<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, &amp; Data Science - Medium]]></title>
        <description><![CDATA[Best practices, tips &amp; tricks from Snowflake experts and community - Medium]]></description>
        <link>https://medium.com/snowflake?source=rss----34b6daafc07---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, &amp;amp; Data Science - Medium</title>
            <link>https://medium.com/snowflake?source=rss----34b6daafc07---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Wed, 01 May 2024 05:34:56 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/snowflake" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Supply Chain Network Optimization on Snowflake ❄️]]></title>
            <link>https://medium.com/snowflake/supply-chain-network-optimization-on-snowflake-%EF%B8%8F-521bfc05d4ce?source=rss----34b6daafc07---4</link>
            <guid isPermaLink="false">https://medium.com/p/521bfc05d4ce</guid>
            <category><![CDATA[snowflake]]></category>
            <category><![CDATA[optimization]]></category>
            <category><![CDATA[linear-programming]]></category>
            <category><![CDATA[mathematical-optimization]]></category>
            <category><![CDATA[supply-chain]]></category>
            <dc:creator><![CDATA[Brett Klein]]></dc:creator>
            <pubDate>Tue, 30 Apr 2024 16:22:12 GMT</pubDate>
            <atom:updated>2024-04-30T16:22:12.178Z</atom:updated>
            <content:encoded><![CDATA[<p>One of the biggest challenges in managing supply chain decisions is the various number of tools required and the technical lift to manage them. In the following guide, we’ll show you how you can drive better supply chain decisions, using linear programming (using <a href="https://coin-or.github.io/pulp/">PuLP</a> in <a href="https://docs.snowflake.com/en/developer-guide/snowpark/python/index">Snowpark</a>), <a href="https://docs.snowflake.com/en/sql-reference/data-types-geospatial">Geospatial Analytics</a>, <a href="https://docs.snowflake.com/en/developer-guide/streamlit/about-streamlit">Streamlit</a>, and <a href="https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions">Snowflake Cortex LLM</a> (large language model) functions, all entirely on Snowflake - no need for <em>any</em> integrations to outside tools.</p><p>In our theoretical scenario, we are managing a North American supply chain consisting of</p><ul><li><strong>10</strong> factories</li><li><strong>25</strong> distributors</li><li><strong>1000</strong> customers</li></ul><p>Each factory has a per-unit production cost and a capacity. Each distributor has a per-unit throughput cost and capacity. Each customer has a set demand. All entities have geographic points associated with them. We are assuming this is all one product (which is easily extendable by adding a new dimension). We are also assuming this is for a fixed timeframe, most likely a monthly or yearly plan.</p><p>Our aim is to answer the questions — <strong>how much product should I make at each factory, and where should I ship it in order to minimize my costs?</strong></p><h3>Getting Started</h3><p>Let’s deploy the solution into your Snowflake environment. The code deploys objects as its own role/database, and so is fully isolated from any other workloads. To deploy:</p><ol><li>Visit the solution’s <a href="https://github.com/Snowflake-Labs/sfguide-supply-chain-network-optimization">github repository</a></li><li>Copy <strong>app_setup.sql</strong>, paste it into a Snowsight SQL worksheet, change your role to ACCOUNTADMIN, and run it</li><li>In Snowsight, set your role to <strong>SCNO_ROLE</strong>, navigate to Projects on the left hand bar, and select Streamlit</li><li>Select <strong>SUPPLY_CHAIN_NETWORK_OPTIMIZATION</strong> in your list of Streamlit apps</li></ol><h4>Data Preparation</h4><p>Before modeling anything, we need our data prepped. To do that, let’s open the <strong>Data Preparation</strong> page from the sidebar in the Streamlit app.</p><figure><img alt="A picture of the Streamlit sidebar, showing the different pages in the app. The pages are Data Preparation, Model Parameters, Model Execution, Model Results, and Cortex Enrichment. There is also a Return Home button at the bottom." src="https://cdn-images-1.medium.com/max/872/1*750ISRIkq1uKtsy5G0xLYA.png" /></figure><p>From the <strong>Data Preparation</strong> page, go ahead and click the <strong>Generate Data</strong> button, leaving the default 1000 customers. This loads the factory and distributor records, and uses Faker to generate customers. Additionally, distances for factories-to-distributors and distributors-to-customers are generated via cross-joins and Snowflake’s <a href="https://docs.snowflake.com/en/sql-reference/functions/st_distance">st_distance</a> function (part of Snowflake’s geospatial analytics capabilities). Freight costs in our model are defined as a premium or discount off of the mileage.</p><p><strong>Model Parameters</strong></p><p>Opening the <strong>Model Parameters</strong> page, we see a dataframe of our factories and all related metadata. At the top, there are tabs that can be used to navigate between different entities/relationships to see all of our input data.</p><h3>Defining an Optimization Model</h3><p>Note — The model in the solution is pre-packaged into the Streamlit, but can be examined by looking at the OptimizationModel class in <strong>supply_chain_network_optimization.py — </strong>we’ll talk through building it.</p><h4>Linear Programming</h4><p>To answer our question, we are going to use <a href="https://en.wikipedia.org/wiki/Linear_programming">linear programming</a> (<em>LP</em>) — also known as constraint programming and linear optimization.</p><p>Linear programming uses a system of inequalities to define a feasible regional mathematical space, and a ‘solver’ that traverses that space by adjusting a number of decision variables, efficiently finding the most optimal set of decisions given constraints to meet a stated objective function.</p><p>It is possible to also introduce integer-based decision variables, which transforms the problem from a <em>linear program</em> into a <em>mixed integer program</em>, but the mechanics are fundamentally the same.</p><p>The <strong>objective function</strong> defines the goal — maximizing or minimizing a value, such as profit or costs. Decision variables are a set of decisions — the values that the solver can change to impact the objective value. The <strong>constraints</strong> define the realities of the business — such as only shipping up to a stated capacity.</p><h4>Objective Functions</h4><p>In our scenario, we have chosen three objectives, each meant to increasingly demonstrate the power of a more representative model.</p><ol><li>Minimize Distance (Crawl) — Emulating a fairly naive present-day. This model ignores any costs, but respects all capacities. This is what may happen if each factory runs as their own business without centralized direction.</li><li>Minimize Freight (Walk) — Emulating an organization with freight analytics capability. This model adds in freight cost, meaning it will look for and exploit freight arbitrages (a cheaper rate for a longer distance). This is what may happen if there is a centralized freight group, but no centralized production or commercial planning.</li><li>Minimize Total Fulfillment (Run) — Emulating a fully realized network analytics capability. This model adds in production and throughput costs for factories and distributors respectively. The model represents the most fully-realized version of the business scenario, balancing all costs together at once.</li></ol><h4>Constraints</h4><p>In order for the models to provide applicable results, we need to constrain the model — a model that minimizes costs by never shipping or producing anything is not a very interesting or useful result! Let’s think of constraints as rules that the model cannot break:</p><ul><li>Factories must ship to distributors first</li><li>All customer demand must be satisfied</li><li>Distributors need to have their inventory replenished</li><li>Factories and distributors cannot ship more than their capacity</li><li>Factories and distributors must ship 0 or more product (positive-bound)</li></ul><h3>Running the Model and Analyzing the Results</h3><p>Returning to Streamlit, let’s move on to the <strong>Model Execution</strong> page. Go ahead and hit the <strong>Run</strong> button. Snowflake will now run the each of the three models, using <a href="https://github.com/coin-or/pulp">PuLP’s</a> included <a href="https://github.com/coin-or/Cbc">CBC Solver</a> all within Snowpark. As the models complete, you’ll see the model name, status, the number of decisions made, and the total cost of the model’s results. You can expand the sample and detail panes if you would like a deeper look.</p><figure><img alt="Streamlit with the Model Execution page, showign results of running the models." src="https://cdn-images-1.medium.com/max/1024/1*e6QeITzyh8buqTckudw1og.png" /></figure><p>Now let’s move on to the <strong>Model Results</strong> page. On this page, you’ll find some filters, dataframes, and maps. Feel free to explore a bit and check out what decisions the models made. Notice how the total fulfillment model outperformed the other two by a pretty significant margin — all without capital investment.</p><figure><img alt="Screenshot of the Streamlit on the Model Results page, showing filters at the top and dataframes with shipment information from the model." src="https://cdn-images-1.medium.com/max/1024/1*c_ufXZGayi-P-NqF3ZE6Cw.png" /></figure><h3>Snowflake Cortex Enrichment</h3><p>Now let’s see how we can leverage Snowflake Cortex’s LLM functionality to get a better picture of our supply chain. Let’s move to the <strong>Cortex Enrichment</strong> page. On this page, you’ll see several attributes that could be helpful for our planning. Go ahead and select whichever you are interested in, and hit the <strong>Enrich</strong> button.</p><figure><img alt="Screenshot of Streamlit with the Cortex Enrichment page. Shows several enrichment options including Closest Airport, Closest Interstate, Closest River, Average Temperature, and Average Rainfall. There is a dataframe that shows factories with the selected field results." src="https://cdn-images-1.medium.com/max/1024/1*l-FBYzZoC5uDeb77lZR35Q.png" /></figure><p>The resulting dataframe includes our factories, and the data we requested — No pipelines or datasourcing needed. The data is sourced from the llama2–70b-chat model, using Snowflake Cortex’s <a href="https://docs.snowflake.com/en/sql-reference/functions/complete-snowflake-cortex">Complete</a> function. Note, no model setup was needed, and the model runs entirely in the context of our account, with no risk of data leakage.</p><p>The method can be extended to any kind of attribute available in publicly available data. If you are curious, you can expand the query pane and see the prompt that was passed for each attribute during the dataframe generation. Feel free to copy/paste/run one of the queries in a new Snowsight SQL worksheet to see it in action.</p><h3>Takeaways</h3><p>We have demonstrated how to store your supply chain data in Snowflake, how to add location and distance-based context using Snowflake’s Geospatial Analytics capabilities, how to build and run linear program models on that data using Snowpark, and how to enrich our supply chain data via Snowflake Cortex’s LLM functions, all wrapped together in a Streamlit-in-Snowflake user interface.</p><p>Additionally, we learned how powerful linear programming can be: with detailed modeling (and honestly-defined constraints), a network optimization model can save <em>significant</em> amounts of money, all without significant capital expenditure. It can even tell you how large the potential opportunity is, so you can dedicate resourcing appropriately.</p><p>The technique outlined in the solution can be extended to many more use cases:</p><ul><li>What-if scenario planning — What do to if a factory is unavailable, where to add capacity?</li><li>Profit maximization modeling — Which customers should I target?</li><li>Goal programming — Does a lower carbon option exist that still meets expense goals?</li><li>Mixed-integer programming — If we give the option for overtime at factories at higher expense, will the model choose to use it and where?</li></ul><p>If you’d like assistance in creating a custom Supply Chain Network Optimization solution, reach out to your Sales team.</p><p><em>Disclaimer: The opinions expressed in this post are my own and not necessarily those of my employer (Snowflake).</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=521bfc05d4ce" width="1" height="1" alt=""><hr><p><a href="https://medium.com/snowflake/supply-chain-network-optimization-on-snowflake-%EF%B8%8F-521bfc05d4ce">Supply Chain Network Optimization on Snowflake ❄️</a> was originally published in <a href="https://medium.com/snowflake">Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, &amp; Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Snowpark ML — An Example Project]]></title>
            <link>https://medium.com/snowflake/snowpark-ml-an-example-project-5627e212520c?source=rss----34b6daafc07---4</link>
            <guid isPermaLink="false">https://medium.com/p/5627e212520c</guid>
            <category><![CDATA[snowpark]]></category>
            <category><![CDATA[snowflake]]></category>
            <dc:creator><![CDATA[Tyler White]]></dc:creator>
            <pubDate>Tue, 30 Apr 2024 15:05:16 GMT</pubDate>
            <atom:updated>2024-04-30T16:07:36.094Z</atom:updated>
            <content:encoded><![CDATA[<h3>Snowpark ML — An Example Project</h3><h4>A brief walkthrough of a template project for Snowpark ML.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cP3M82jPEwPpgT9K7C341A.png" /><figcaption>DALL·E 3: A colorful, fictional world of the 1800s, featuring a polar bear and two human data scientists.</figcaption></figure><h3>Background</h3><p>The Snowpark ML Modeling API has been generally available since early December 2023, and the Snowpark Model Registry has been in public preview since January 2024.</p><p>There are a few examples of working with these packages out there that we frequently visit for inspiration and reference. <a href="https://medium.com/u/bc62ab47b4a9">Chase Romano</a> and <a href="https://medium.com/u/829dc2ec3eae">Sikha Das</a> have shared some great material on this subject.</p><ul><li><a href="https://github.com/cromano8/Snowflake_ML_Intro">GitHub - cromano8/Snowflake_ML_Intro: Introduction to performing Machine Learning on Snowflake</a></li><li><a href="https://quickstarts.snowflake.com/guide/intro_to_machine_learning_with_snowpark_ml_for_python">Intro to Machine Learning with Snowpark ML</a></li></ul><p>These examples use notebooks, which are great but can often be challenging to maintain in repositories. We frequently encounter scenarios where users want to keep their code in Python scripts and operationalize it via traditional orchestration tools and/or Snowflake tasks.</p><p><a href="https://medium.com/u/3eeeaa27a8c9">Kirk Mason</a> and I opted to implement our project as a Python module. This helps avoid needing to append a module path to the system path to specify imports, which we often see in notebooks. The scripts we will walk through can be executed directly in Python or registered via Snowpark as Stored Procedures, later executed as Snowflake tasks in a DAG.</p><p>We think having a well-organized project structure is essential, so that’s what we’ll outline here.</p><h3>The Project Structure</h3><p>Having a template or framework is essential to start any project. This helps with onboarding new team members as well. While there may be variations of this to fit specific needs, here’s an outline of what we find to be a good starting point.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/944/1*TALLlvyL3VHz7NayJjXT4w.png" /><figcaption>Example Project Structure</figcaption></figure><p>Let’s break down these sections and the various files at the top level.</p><h4>docs</h4><p>The docs folder contains any project-specific documentation needed to encompass requirements and help enable future developers and stakeholders.</p><h4>scratch</h4><p>The scratch folder contains any notebook code, experimentation, or exploration that should not be executed automatically.</p><h4>my_project (or src)</h4><p>This folder serves as the “heart” of our project. Treating code as a Python package has several advantages. It enhances modularity, organization, and readability by separating related functionalities into distinct modules. It also provides namespace isolation to avoid naming clashes and promotes reusability, minimizing code duplication.</p><p>I mentioned in the earlier section that providing options to run these Python scripts directly or by registering the functions was important. Here’s how we did that. The code is shortened for brevity, as the model training code isn’t the main focus here.</p><pre>import numpy as np<br>from snowflake.ml.modeling.impute import SimpleImputer<br>from snowflake.ml.modeling.pipeline import Pipeline<br>from snowflake.ml.modeling.xgboost import XGBRegressor<br>from snowflake.ml.registry import Registry<br>from snowflake.snowpark import Session<br>from snowflake.snowpark import functions as F<br>from snowflake.snowpark import types as T<br>from my_project.common import get_next_version<br>from my_project.common import get_performance_metrics<br><br>import logging<br><br><br>def train(session: Session) -&gt; str:<br>    logger = logging.getLogger(__name__)<br>    logger.info(<br>        &quot;{&#39;message&#39;:&#39;Begin model training procedure&#39;, &#39;unit&#39;:&#39;analytics&#39;}&quot;<br>    )<br>    ...<br><br>    pipeline = Pipeline(<br>        [<br>            ...<br>        ]<br>    )<br>    pipeline.fit(train_df)<br><br>    logger.info(<br>        &quot;{&#39;message&#39;:&#39;Obtain metrics&#39;, &#39;unit&#39;:&#39;analytics&#39;}&quot;<br>    )<br><br>    train_result_df = pipeline.predict(train_df)<br>    test_result_df = pipeline.predict(test_df)<br><br>    combined_metrics = dict(<br>        train_metrics=get_performance_metrics(<br>            &quot;REGRESSION&quot;, train_result_df, &quot;PRICE&quot;, &quot;OUTPUT_PRICE&quot;<br>        ),<br>    )<br><br>    reg = Registry(session=session, schema_name=&quot;MODELS&quot;)<br><br>    model_name = &quot;MY_MODEL&quot;<br>    model_version = get_next_version(reg, model_name)<br><br>    reg.log_model(<br>        model_name=model_name,<br>        version_name=model_version,<br>        model=pipeline,<br>        metrics=combined_metrics,<br>    )<br><br>    logger.info(<br>        &quot;{&#39;message&#39;:&#39;Finished training and registering&#39;, &#39;unit&#39;:&#39;analytics&#39;}&quot;<br>    )<br><br>    return f&quot;Model {model_name}.{model_version} is trained and deployed.&quot;<br><br><br>if __name__ == &quot;__main__&quot;:<br>    session = Session.builder.getOrCreate()<br>    session.use_warehouse(&quot;ML_BIG&quot;)<br>    session.use_database(&quot;ML_EXAMPLES&quot;)<br>    session.use_schema(&quot;DIAMONDS&quot;)<br>    raise SystemExit(train(session))</pre><p>We define our function train, which can be registered directly as a stored procedure at a later step (<strong>register_deploy_dags</strong>).</p><p>As an alternative to registering as a stored procedure, invoking this script directly will hit the __main__ condition, establish a connection to Snowflake, and execute the train function. This allows using this code with an orchestration tool if you do not intend to use Snowflake tasks.</p><h4>pyproject.toml</h4><p>The pyproject.toml file contains the information needed to set up the project and the Python environment. Here’s what ours looks like.</p><pre>[build-system]<br>requires = [&quot;setuptools&quot;, &quot;wheel&quot;]<br>build-backend = &quot;setuptools.build_meta&quot;<br><br>[project]<br>name = &quot;my_project&quot;<br>description = &quot;A Snowpark ML project.&quot;<br>version = &quot;0.1.0&quot;<br>readme = &quot;README.md&quot;<br>dependencies = [<br>    &quot;snowflake-snowpark-python==1.14.0&quot;,<br>    &quot;numpy==1.26.3&quot;,<br>    &quot;scikit-learn==1.3.0&quot;,<br>    &quot;snowflake[ml]&quot;,<br>    &quot;xgboost==1.7.3&quot;<br>]<br><br>[tool.setuptools.packages.find]<br>include = [&quot;my_project&quot;]<br><br>[project.optional-dependencies]<br>dev = [&quot;nbqa[toolchain]&quot;, &quot;jupyter&quot;]</pre><p>Having this file in the root of our repo will allow us to easily install our project with pip by executing the following command:</p><pre>pip install .</pre><p>Or, if you wish to perform an editable install, you can also do the following:</p><pre>pip install -e .</pre><p>Since we have specified a “dev” extra in our configuration file, we can also install dev dependencies like this:</p><pre>pip install -e &quot;.[dev]&quot;</pre><h4>register_deploy_dags.py</h4><p>In this code, we’re registering stored procedures from our Python module and setting them up to create a DAG in Snowflake.</p><pre>session.sproc.register_from_file(<br>    file_path=&quot;my_project/train.py&quot;,<br>    func_name=&quot;train_model&quot;,<br>    name=&quot;TRAIN_MODEL&quot;,<br>    is_permanent=True,<br>    packages=[&quot;snowflake-snowpark-python&quot;, &quot;snowflake-ml-python&quot;],<br>    imports=[&quot;my_project&quot;],<br>    stage_location=&quot;@PYTHON_CODE&quot;,<br>    replace=True,<br>    execute_as=&#39;caller&#39;<br>)</pre><p>Later, we can deploy our DAG in the same script.</p><pre>with DAG(<br>    &quot;EXAMPLE_DAG&quot;,<br>    schedule=Cron(&quot;0 8 * * 1&quot;, &quot;America/New_York&quot;),<br>    stage_location=&quot;@PYTHON_CODE&quot;,<br>    use_func_return_value=True,<br>) as dag:<br>    train_task = DAGTask(<br>        name=&quot;TRAIN_MODEL_TASK&quot;,<br>        definition=&quot;CALL TRAIN_MODEL();&quot;,<br>        warehouse=&quot;COMPUTE_WH&quot;,<br>    )<br>    set_default_task = DAGTask(<br>        name=&quot;SET_DEFAULT_VERSION&quot;,<br>        definition=&quot;CALL SET_DEFAULT_VERSION(&#39;DIAMONDS&#39;, &#39;rmse&#39;, True);&quot;,<br>        warehouse=&quot;COMPUTE_WH&quot;,<br>    )<br>    train_task &gt;&gt; set_default_task</pre><p>If you want to learn more about how this works, please check out the <a href="https://docs.snowflake.com/developer-guide/snowflake-python-api/snowflake-python-managing-tasks">docs</a>.</p><h3>Conclusion</h3><p>We hope this gives you a foundation for getting started and is flexible enough to meet your needs.</p><p>We encourage you to try this, make any necessary changes, and let us know how it works. Please don&#39;t hesitate to share your experiences or challenges, as they might help others.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5627e212520c" width="1" height="1" alt=""><hr><p><a href="https://medium.com/snowflake/snowpark-ml-an-example-project-5627e212520c">Snowpark ML — An Example Project</a> was originally published in <a href="https://medium.com/snowflake">Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, &amp; Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Snowflake Dynamic Tables in Lieu of Traditional Materialized Views]]></title>
            <link>https://medium.com/snowflake/using-snowflake-dynamic-tables-in-lieu-of-traditional-materialized-views-2bcc29d7a654?source=rss----34b6daafc07---4</link>
            <guid isPermaLink="false">https://medium.com/p/2bcc29d7a654</guid>
            <category><![CDATA[snowflake]]></category>
            <category><![CDATA[dynamic-table]]></category>
            <category><![CDATA[data-superhero]]></category>
            <dc:creator><![CDATA[Mike Taveirne]]></dc:creator>
            <pubDate>Tue, 30 Apr 2024 15:04:41 GMT</pubDate>
            <atom:updated>2024-04-30T15:04:41.668Z</atom:updated>
            <content:encoded><![CDATA[<p>My use case is essentially seeking a traditional materialized view, although Dynamic Tables do so more efficiently — and beyond what Snowflake itself deems materialized views.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*qviQ-gUib6im3E2mTDHuhQ.jpeg" /></figure><p>To simplify my case, let’s say I’m simply looking for the lifetime sales of all of my customers and with some basic aggregations. Sum of sales, count of orders, average order size. A full table scan and aggregation across my entire transaction table.</p><h3>But Wait, Doesn’t Snowflake Already Have Materialized Views?</h3><p>Mmm yes and no — here’s the doc around a traditional MV in <a href="https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/CREATE-MATERIALIZED-VIEW.html">Oracle</a>. You can see it is the results of a <em>query</em>. The query can arbitrarily be everything. The view is costly in that it is essentially a static table until it is refreshed, and the refresh updates the entire table. If I have 100 million customers and 1 made a new order, the MV is now out of date, and if I refresh it — it is going to calculate the aggregate for all of my customers. Quite costly!</p><p>Here we can see the doc for <a href="https://docs.snowflake.com/en/user-guide/views-materialized">Snowflake’s materialized views</a>. However you will note it is not simply a query — as it is limited to a single table. It is a bit more of a performance nuance around usage of a table. It could contain a subset of columns and already filtered data from the table it is based on; it may include aggregates as well. This MV could be directly queried for results. Snowflake does seem to do a good job of processing changed data incrementally for these.</p><p>However it can be used even when not directly reference in a query — the projected and restricted rows, along with potentially different cluster approach — may cause the SQL optimizer to find the MV more useful to satisfy a query than the base table itself. Thus it need not be referenced to be used for performance gains. I might refer to such a table as more of a “Materialized Columnar Projection” than a traditional MV, because it is functioning as an alternate version of the single base table that may be optimal for some query patterns.</p><h3>So What’s New With Dynamic Tables?</h3><p>With Dynamic Tables, we now have the ability for the query to support <em>any arbitrary SQL</em>. Note — ideally our query is such that it is fully deterministic and can be updated incrementally. See the doc <a href="https://docs.snowflake.com/en/user-guide/dynamic-tables-refresh#label-dynamic-tables-intro-refresh-modes">here</a> for refresh modes; doing a lifetime aggregation we should be able to take advantage of incremental refreshes of only changed customers as their orders are brought into our source table. Note this would be different if we were trying to find the same metrics for the last 30 days, and doing a filter on something like the current_date — interval &#39;30 days&#39; which would require a full refresh! Sample:</p><pre>CREATE OR REPLACE DYNAMIC TABLE miket_data.public.dynamic_table_lifetime_aggs <br>TARGET_LAG = &#39;1 day&#39;<br>WAREHOUSE = MY_WH<br>AS<br>    SELECT o_custkey<br>    , sum(o_totalprice) as lifetime_sum<br>    , count(1) as lifetime_count<br>    , avg(o_totalprice) as lifetime_avg<br>    , max(o_orderdate) as o_lastorder<br>    from miket_data.public.orders_1<br>    group by o_custkey<br>;   </pre><p>Note because Snowflake adds change tracking to underlying tables/views, I was unable to reference the shared dataset directly to add this due to limited privileges against Snowflake’s snowflake_sample_data.tpch_sf1.orders. I made a view in my own schema on it instead, which could be watched for changes. In my case this is an aggregation done in bulk load after a day’s transaction loading is complete, so there is no need to refresh it more than daily.</p><p>Given Snowflake is simply letting me set a TARGET_LAG here, likely to result in a sliding &lt; 24h schedule at some point of the day outside of my control — I’ll always just run my own manual refresh of the table anyway. After the source has the prior day’s data inserted via ETL/ELT, I’ll simply call the refresh command.</p><pre>alter dynamic table miket_data.public.dynamic_table_lifetime_aggs refresh;</pre><p>This will go ahead and incrementally update only those customers who have experienced a change in the data via new orders coming in for them. It doesn’t really matter when Snowflake also decides to run its 1-day target job — it will find there’s nothing to do as no new data has arrived and won’t be arriving until the next day’s ETL/ELT job from the operational system of record.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*63hpDwy27GtL_8kOFNaZAg.jpeg" /></figure><h3>Best of Both Worlds!</h3><p>So what do we end up with in the end? We have an object that has all the advantages of a traditional materialized view, plus the ability to incrementally update only changed records so that it is constructed in the most efficient way. Awesome!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=2bcc29d7a654" width="1" height="1" alt=""><hr><p><a href="https://medium.com/snowflake/using-snowflake-dynamic-tables-in-lieu-of-traditional-materialized-views-2bcc29d7a654">Using Snowflake Dynamic Tables in Lieu of Traditional Materialized Views</a> was originally published in <a href="https://medium.com/snowflake">Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, &amp; Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Snowflake Copilot: Exciting News — It’s Here!]]></title>
            <link>https://medium.com/snowflake/snowflake-copilot-exciting-news-its-here-04e1bb0974a5?source=rss----34b6daafc07---4</link>
            <guid isPermaLink="false">https://medium.com/p/04e1bb0974a5</guid>
            <category><![CDATA[data-superhero]]></category>
            <category><![CDATA[llm]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[snowflake]]></category>
            <dc:creator><![CDATA[Alexander]]></dc:creator>
            <pubDate>Tue, 30 Apr 2024 14:59:13 GMT</pubDate>
            <atom:updated>2024-04-30T14:59:12.985Z</atom:updated>
            <content:encoded><![CDATA[<h3>Snowflake Copilot: Exciting News — It’s Here!</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/512/1*sfVZax_Q5dp_JZS0NAzAzw.png" /><figcaption>Source: AI Generated Image</figcaption></figure><p>Finding it tough to remember SQL syntax?</p><p>Uncertain about your database’s data?</p><p>Wondering how to derive meaningful business insights and KPIs from your schema?</p><p>And if you’re new to Snowflake, writing optimized queries might seem like a challenge, right?</p><p>then answer would be enter <strong>Snowflake Copilot</strong> — a game-changer in the world of data management and analysis in snowflake. Today, I had the exciting opportunity to dive into Snowflake Copilot during its exclusive preview phase, available in just two regions. Let me tell you, It’s like having a helpful assistant for your data tasks.</p><p>Imagine a world where you no longer need to struggle with syntax or guess the contents of your database. With Snowflake Copilot, you can unlock the full potential of your data, effortlessly extracting valuable insights and KPIs to drive your business forward.</p><p>Whether you’re a Snowflake pro or a beginner, Snowflake Copilot makes data easier. It’s time to ditch the uncertainty and embrace better data management with Snowflake Copilot!</p><p>In this blog, I’ve shared my initial thoughts about Snowflake Copilot.</p><blockquote><em>What is Snowflake copilot?</em></blockquote><p>Well, it’s like having a supercharged assistant right within your Snowflake setup. This powerful tool, driven by LLM technology, is all about transforming how you work with databases. It makes analyzing data a breeze, helping you get the insights you need faster and with more depth.</p><p>Just picture it: You can ask questions directly to your Snowflake database and schema, and boom — get instant results. And here’s the kicker: Snowflake Copilot can even write SQL queries for you, including optimized ones if needed. It’s like having a data analysis wizard by your side, making the process smoother than ever.</p><blockquote><strong>In which regions is Snowflake Copilot available?</strong></blockquote><p>As of now, Snowflake Copilot is available in just two regions:</p><ul><li>AWS us-east-1</li><li>AWS us-west-2</li></ul><blockquote>What are the costs associated with it?</blockquote><p>As of now, there are no costs associated for interacting with Snowflake Copilot. However, if you run the queries generated by Snowflake Copilot within the engine, the warehouse costs apply as usual.</p><blockquote>How to Set Up Snowflake Copilot?</blockquote><p>After creating an account in the specified regions, when you select or create a session, Snowflake Copilot becomes available in the bottom right corner. Check out the screenshot below, where “<strong>ASK Copilot</strong>” is highlighted in the red box.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HKZtTew2M5JiCS_jHbKZWw.png" /><figcaption><strong>Fig 1 — Snowflake Copilot</strong></figcaption></figure><p>Clicking on “<strong>Ask Copilot</strong>” gives you the option to select the database and schema you want to query. From there, you can inquire about tables or ask questions related to the database and schema.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/464/1*HPBhVIiDxaguU9kSpHVLHw.png" /><figcaption><strong>Figure 2 — Copilot Workspace</strong></figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/777/1*4JEL5QTfqI2YMnprkhgi4g.png" /><figcaption><strong>Fig 3 — Database and Schema selection in CoPilot workspace</strong></figcaption></figure><blockquote>Here are the cutting-edge advantages I’ve experienced while using Snowflake Copilot:</blockquote><ol><li><strong>Seamless dataset exploration: </strong>By asking plain English questions to Snowflake Copilot, we can effortlessly dive into our datasets.</li></ol><p><strong>For example: </strong>When prompted for a specific table, Snowflake Copilot intelligently discusses that table along with other relevant ones to derive valid KPIs.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/388/1*ij3zw8Uy2ha0t7YUs8XYBw.png" /><figcaption><strong>Fig 4 — Dataset Exploration</strong></figcaption></figure><p>2. <strong>Transforming Business Questions:</strong> With just one click, your business questions can be transformed into Snowflake queries and seamlessly embedded into the worksheet, as illustrated below,</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LUcZ3Z35uAmClMY2NL-Zug.png" /><figcaption>Fig 5 — Transformation of Business Questions to SQL Queries with Steps</figcaption></figure><p>Here’s a step-by-step guide on using Snowflake Copilot:</p><p><strong>Step 1:</strong> Choose the database and schema where you want to run your query.</p><p><strong>Step 2:</strong> Phrase your question in plain English. For instance, you might ask, “Determine the number of orders placed by each customer, sorting them in descending order based on both the number of orders and the total amount spent by the customer.”</p><p><strong>Step 3: </strong>Snowflake Copilot provides a detailed explanation based on your question and generates the corresponding SQL query.</p><p><strong>Step 4: </strong>There are two buttons available:</p><ul><li><strong>Add Button:</strong> Adds the SQL query to the worksheet without executing it against the database. Comments are also highlighted.</li><li><strong>Run Button:</strong> Adds the SQL query to the worksheet and executes it against the database. This step is highlighted in Step 5.</li></ul><p><strong>Step 5: </strong>Upon clicking the “Run” button, the SQL query is added to the worksheet and automatically executed against the database.</p><p><strong>Step 6:</strong> The result set of your business question is displayed.</p><p><strong>3. Validating Queries:</strong> If a query generated by Snowflake Copilot is invalid, it will be highlighted within the tool, as shown in the figure below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/389/1*SfU-GYq9qAqwsANRtUixcg.png" /><figcaption>Fig 6 — SQL Query Validation</figcaption></figure><blockquote>Some Limitations Encountered with Snowflake Copilot:</blockquote><ul><li><strong>Cross-database and schema</strong> queries are not supported. To overcome this, alternative methods like creating views can be used for querying within Snowflake.</li><li>Newly created databases and schemas may take some time to be reflected in Snowflake Copilot. If queried immediately after creation, you may encounter a message stating,</li></ul><p>“We are in the process of indexing the tables and views in the selected database and schema. Please try again later.”</p><ul><li>Occasionally, Snowflake Copilot may take some time to generate a response.</li><li>The SQL queries generated by Copilot may not always be 100% accurate, requiring tuning based on the output. However, overall functionality remains efficient.</li><li>When listing tables for analysis, Copilot provides results for the top 10 tables and top 10 columns from each of those tables.</li></ul><p><strong><em>References: -</em></strong></p><ul><li><a href="https://www.snowflake.com/">https://www.snowflake.com/</a></li></ul><p><strong><em>About me:</em></strong></p><blockquote><em>I am a Cloud Data Architect with experience as a Senior Consultant at EY New Zealand. Throughout my career, I have worked on numerous projects involving legacy data warehouses, big data implementations, cloud platforms, and migrations. If you require assistance with certification, data solutions, or implementations, please feel free to connect with me on </em><a href="https://www.linkedin.com/in/alexander-murugesan-17472295/"><em>LinkedIn</em></a><em>.</em></blockquote><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=04e1bb0974a5" width="1" height="1" alt=""><hr><p><a href="https://medium.com/snowflake/snowflake-copilot-exciting-news-its-here-04e1bb0974a5">Snowflake Copilot: Exciting News — It’s Here!</a> was originally published in <a href="https://medium.com/snowflake">Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, &amp; Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Introducing CAPS, your Snowflake Arctic-powered Retail AI Agent]]></title>
            <link>https://medium.com/snowflake/introducing-caps-your-snowflake-arctic-powered-retail-ai-agent-24b9bf9cb117?source=rss----34b6daafc07---4</link>
            <guid isPermaLink="false">https://medium.com/p/24b9bf9cb117</guid>
            <category><![CDATA[arctic]]></category>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[cortex]]></category>
            <category><![CDATA[llm]]></category>
            <category><![CDATA[snowflake]]></category>
            <dc:creator><![CDATA[Jonathan L Tao]]></dc:creator>
            <pubDate>Mon, 29 Apr 2024 20:42:50 GMT</pubDate>
            <atom:updated>2024-04-29T21:12:57.994Z</atom:updated>
            <content:encoded><![CDATA[<h4>A Snowflake Cortex Arctic Powered Shopper</h4><p><em>In collaboration with </em><a href="https://medium.com/u/af0b70fbc112"><em>Chinmayee Lakkad</em></a><em> and </em><a href="https://medium.com/u/bf7567cdeda0"><em>Zohar Nissare-Houssen</em></a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/936/0*GnW6SIpmWL3Bc0V_" /></figure><h3>Preamble</h3><p><em>By </em><a href="https://medium.com/u/bf7567cdeda0"><em>Zohar Nissare-Houssen</em></a></p><p>You may have heard a variation of this joke:</p><blockquote>A software tester strolls into a bar and orders a series of valid and nonsensical beers from the bartender. As the bartender navigates the absurd requests gracefully, a real customer innocently asks for directions to the restroom, triggering a catastrophic chain of events that ends in chaos.</blockquote><p>Now, fast forward to the era of Generative AI-powered chatbots, where the bartender is the AI Agent behind your chatbot. Just like the bartender, the AI Agent must deftly handle diverse consumer inquiries, discerning valid requests from gibberish. Your bartender has an additional challenge in the fact that he may not know all the information required to address the customer inquiry. He may need to retrieve some information from the right source and retrieve it adequately — whether through SQL queries, semantic searches, or a combination of both.</p><p>In this blog, we review some key concepts and best practices in implementing a custom AI Agent using Snowflake Cortex capabilities, called CAPS (Cortex Arctic Powered Shopper). Snowflake is your one-stop shop to build your custom AI Agents from the ground up within the security perimeter of your Snowflake Account, where all your data resides.</p><p>Building upon the groundwork laid for an AI Agent in the <a href="https://medium.com/snowflake/clinical-trial-copilot-a-rag-based-approach-on-snowflake-leveraging-cortex-capabilities-series-ef78cba5b59d">Clinical Trial Assistant</a> by myself and <a href="https://medium.com/u/96e83e83cc0">Harini Gopalakrishnan</a>, CAPS pushes some concepts further and shows its application in the retail vertical. In addition, CAPS showcases the latest Snowflake provided enterprise-grade open source LLM, <a href="https://www.snowflake.com/en/data-cloud/arctic/">Snowflake Arctic</a>, and applies it to solve a common Retail Industry problem.</p><p>In this blog, we delve into a practical approach and best practices to ensure seamless consumer interactions and optimal information retrieval through a custom AI Agent.</p><h3>Step 0: Data Pre-processing and Data Preparation</h3><p>Before we begin the AI Agent development, we need to obtain an appropriate dataset to work against. <a href="https://oxylabs.io/">Oxylabs</a> provides a sample dataset of Amazon products free on the <a href="https://www.snowflake.com/en/data-cloud/marketplace/">Snowflake Marketplace</a> and is perfect for our needs. Each row has a json payload of many of the attributes findable on an Amazon product page listing (title, department, quantity, price, description, etc.), which is a great starting point for simulating inventory of an e-commerce store.</p><h4>Using LLMs early on</h4><p>One of the wrinkles of working with Amazon product titles is the verbosity that is associated with these items. Anyone familiar with browsing Amazon has seen product titles like this:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/513/0*1too46psMp66q2T0" /><figcaption>A typical product title from your favorite e-commerce website.</figcaption></figure><p>These titles are provided in the Oxylabs dataset, but they would not be very conversational if returned verbatim. We can leverage LLM capabilities at this stage by issuing a <a href="https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions#label-cortex-llm-complete">Cortex Complete</a> function and prompting the LLM to simplify the item description. We do this at the same time we transform the json payload into a more structured result for ease of use. Another normalization step we take is isolating a distinct list of departments in a dimension table — this will be leveraged later.</p><h4>Vector Data Type and Response Augmented Generation (RAG)</h4><p>LLMs by themselves pack a punch when it comes to innate knowledge. But this knowledge does not expand to first party data which needs to be closely governed. In such cases, LLMs could either be retrained, fine-tuned, or given that extra bit of information on the fly, so that they may produce the relevant answers every time. Of these three techniques, the last one that has become most popular for its cost efficiency and efficacy and is known as Retrieval Augmented Generation (abbreviated RAG, for a specific overview of RAG, refer to <a href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/">this</a> article).</p><p>The idea is really simple — compliment LLM’s knowledge with the right information in the moment of need (read when LLM is queried). The right information here is pulled from a given corpus of information, hereto unseen by the LLM, using Vector semantic search. Typically, RAG enabled LLM applications use a Vector database to store such data types with operations on top for semantic similarity search &amp; retrieval. The actual data stored in vector data types is called vector embeddings, a mathematical representation of text data/sentences in higher dimensional space.</p><p>Snowflake, in keeping with its “Ease of Use” design philosophy in mind, has simplified how vector embeddings are generated and stored — through providing a native column data type called “Vector.” Snowflake also provides operations to generate vector embeddings from text data that can be captured in this type of column and sit right alongside the original text, in the same table object.</p><p>With CAPS, we’ve created vector embeddings for the product title given that end users are likely to search for products using some form of description. Product titles here are treated as short descriptions of what they are supposed to be. After normalization and cleanup of the data, we alter the table to add this new vector column and store the generated embeddings in it.</p><p>The stage now is set for CAPS to come together.</p><h3>Step 1: Establishing the LLM Inference Points and App Architecture</h3><p>Now that our data is prepared, we need to map out the conversational experience that CAPS will be programmed to support. Below are the key objectives:</p><ol><li>Handle human-generated questions in a chat interface</li><li>Ascertain when questions are related to retail and e-commerce</li><li>Leverage Retrieval Augmented Generation to provide item suggestions or details on a specific item or do a direct query filter based search if the question is about a specific item.</li></ol><p>To facilitate this flow of questions, we need to implement the following concepts:</p><h4>Intent Classification</h4><p><a href="https://www.helpshift.com/glossary/intent-classification/#:~:text=In%20the%20world%20of%20artificial,for%2C%20well%2C%20order%20status.">Intent classification</a> is a key concept when interacting with AI Agent as it helps guide the LLM into generating the proper response.<strong> </strong>In addition to answering questions directly, we will need to have the LLM perform intent classification in two stages:</p><ul><li><strong>Problem 1:</strong> Determine if the question is related to our retail scenario or not. This directly controls whether we provide a RAG-based response.</li><li><strong>Problem 2: </strong>If the question is retail in nature, determine if the question pertains to a specific item or not, while considering the chat history. This will be referred to as “direct lookup”.</li></ul><h4>RAG-based Concepts</h4><p>So far, we’ve discussed how to prepare for RAG by leveraging Snowflake’s Vector data type; however, there are other elements of RAG that we will leverage in facilitating CAPS’ conversational capability. Below are specific examples and expansion on RAG concepts used in development.</p><ul><li><strong>Cosine Similarity </strong>: From the previous section, what RAG does is provide additional material to an LLM “on-the-fly” by inserting into its prompt context. The relevant information itself is picked from the corpus (like our prepped data) using vector semantic search. Semantic search compares the user’s question to existing vector embeddings using a method called <a href="https://www.geeksforgeeks.org/cosine-similarity/">cosine similarity</a> which provides a score based on how “close” (in meaning/semantic) the embeddings are to the question. The greater the score, the greater the similarity. In our example we look for similar embeddings of product titles with score over 0.79 (inclusive). This route is taken when the first two LLMs in the chain assess that the question was a retail question but not related to a specific product.</li><li><strong>Chat History</strong>: While chat history is not stored in the database in our example, it is recorded using Streamlit’s <a href="https://docs.streamlit.io/develop/api-reference/caching-and-state/st.session_state">session_state</a> feature which adds statefulness to any Streamlit app. We record both the user and CAPS’ responses sequentially, and allow prompts to consider this chat history where appropriate.</li><li><strong>Direct Lookup</strong>: In the event where the question is determined to be about a specific item, CAPS is instructed to extract the ASIN value of the item in question. This is aided by chat history since the ASIN value is provided alongside any item suggested and is contextually available to the LLM. With this context plus the guidance of our intent classifier, the app is able to return a specific ASIN value; therefore, direct lookup becomes a trivial SQL operation and CAPS can respond appropriately.</li></ul><h4>Conversation Architecture</h4><p>With all of these concepts in consideration, we arrive at the following flow diagram:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*PJksnMM7f3RZYgVv" /><figcaption>Conversational flow diagram of CAPS AI Agent</figcaption></figure><p>Each key task, or inference point, in this diagram represents a python function centered around a <a href="https://docs.snowflake.com/en/sql-reference/functions/complete-snowflake-cortex">Snowflake Cortex Complete</a> call. Prompt engineering does the heavy lifting here, guiding the LLM’s output appropriately.</p><p>What’s beneficial about this architecture is that you can plug-and-play different Cortex-provided LLM models at different inference points, to — (1) adjust for optimal LLM size sufficient for accuracy at that point, and (2) adjust price/performance ratio based on your SLA for budget.</p><p>Regarding efficiency, we’re also leveraging SQL filters to subset the data and then RAG with semantic search on top. Both address problems 1 and 2 above by filtering on department first and then specific product id, respectively.</p><h3>Step 2: Putting it All Together</h3><p>Using Streamlit in Snowflake, the development workflow for building chatbots is vastly simplified. The Streamlit App coding environment standardizes the dependencies for Python, Snowpark functions, as well as Streamlit while providing side-by-side interactivity with your application in real-time.</p><p>With this approach laid out so far, we arrive at a completed AI Agent. Let’s take a look at a quick conversation with CAPS.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/758/1*9Pytny6k-O2OhzMCziUwgA.gif" /><figcaption>A conversation with CAPS</figcaption></figure><h3>Conclusion and Looking Forward</h3><p>Overall, creating a context-aware conversational AI Agent purely on Snowflake was at most a few days effort (including the data preparation) and extremely streamlined.</p><p>In addition to the development ease of use, it’s important to call out other benefits of a Snowflake-based, single platform AI Agent.</p><ul><li><strong>Data never leaves the platform:</strong> This is a key consideration when companies are trying to leverage LLMs against their data in a RAG-based fashion. Snowflake does this natively by bringing first-in-class LLMs to your data, inside the platform. Customers benefit from Snowflake’s powerful data governance and security characteristics; data never leaves the platform while being served through AI Agent.</li><li><strong>Snowflake Cortex is a managed service: </strong>In harmony with much of Snowflake’s vision, Cortex is delivered in a managed service way. No infrastructure management is needed to leverage Cortex functionality, and consumption-based usage is implemented as opposed to paying for cluster uptime. Customers benefit from cost savings and usage of AI/ML capabilities, regardless of use case size or scale.</li><li><strong>All inference points are using Snowflake Arctic:</strong> Snowflake’s own LLM, <a href="https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/">Arctic</a>, was released very recently, and proves extremely useful in this enterprise-AI use case.</li></ul><p>CAPS was created using the currently available capabilities of Cortex service, with the exception of Vector Search entering Public Preview shortly. That being said, additional capabilities and considerations around RAG-based model training as well as model selection are upcoming and should be considered for the future state of the AI Agent, as explained below.</p><h4>Furthering of Cortex Capabilities</h4><p>Cortex has been available since November 2023 and continues to be an evolving part of the Snowflake platform. LLMs continue to build upon RAG-based performance and accuracy with new search and learning methodologies. As Snowflake also incorporates these methods into the Cortex functional catalog, we will revisit their efficacy in CAPS as needed.</p><h4>Multimodal Capabilities with Reka AI</h4><p>Snowflake continues to strategically integrate new LLMs into the Snowflake Cortex <a href="https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions#choosing-a-model">portfolio of models</a>. One of the most recent integrations is <a href="https://www.reka.ai/news/reka-flash-efficient-and-capable-multimodal-language-models">reka-flash</a>, an efficient multimodal and multilingual model. While the Cortex Complete function is limited to text completion today, multimodal capabilities provide opportunity to leverage this model to further use cases in retail, such as generating descriptions based on product images.</p><h4>Furthering RAG capabilities with Graph databases</h4><p>The current agent is able to leverage SQL and semantic search capabilities native to Snowflake which provides reasonable performance. However, for more complex product nomenclatures in retail, and richer semantics, we do need to explore the addition of graph databases within the RAG framework of the agent.</p><p>In conclusion, the value add of CAPS or related chatbot across industries should not be underestimated, and we hope this blog has inspired you to activate your Snowflake data in kind.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=24b9bf9cb117" width="1" height="1" alt=""><hr><p><a href="https://medium.com/snowflake/introducing-caps-your-snowflake-arctic-powered-retail-ai-agent-24b9bf9cb117">Introducing CAPS, your Snowflake Arctic-powered Retail AI Agent</a> was originally published in <a href="https://medium.com/snowflake">Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, &amp; Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Turning a Streamlit in Snowflake into a Native App — My April Month Goal]]></title>
            <link>https://medium.com/snowflake/turning-a-streamlit-in-snowflake-into-a-native-app-my-april-month-goal-9e30fe8bb64c?source=rss----34b6daafc07---4</link>
            <guid isPermaLink="false">https://medium.com/p/9e30fe8bb64c</guid>
            <category><![CDATA[snowflake]]></category>
            <category><![CDATA[app-development]]></category>
            <category><![CDATA[apps]]></category>
            <category><![CDATA[streamlit]]></category>
            <category><![CDATA[native-app]]></category>
            <dc:creator><![CDATA[Marianne Voogt]]></dc:creator>
            <pubDate>Mon, 29 Apr 2024 15:51:38 GMT</pubDate>
            <atom:updated>2024-04-30T09:15:25.090Z</atom:updated>
            <content:encoded><![CDATA[<h3>Turning a Streamlit in Snowflake into a Native App — My April Month Goal</h3><p>As a part of my April Month Goal, I challenged myself to build something. What’s a better way than to try building a Snowflake Native App!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*CrgJZYFwgK8FhmoLukwb9A.jpeg" /><figcaption>Journalling for my April Month Goal (generated with Playground.com)</figcaption></figure><p>Quite a few of my customers have taken the step from standalone Streamlit applications to Streamlit-in-Snowflake for internal apps since Streamlit in Snowflake went GA. Taking that Streamlit-in-Snowflake application and turning it into a Native App is an opportunity for those customers to manage those Streamlit applications centrally and distribute them across Snowflake accounts in their organization, as Brian Hess outlined in his article <a href="https://medium.com/snowflake/snowflake-native-apps-3-less-obvious-use-cases-37b3b68e8107">‘Snowflake Native Apps — 3 Less Obvious Use Cases’ </a>.</p><p>And then, once you are ready to embark on <a href="https://medium.com/snowflake/unlocking-revenue-streams-4-use-cases-for-monetizing-with-the-snowflake-native-app-framework-c70443b52d98">the journey of monetization</a> (a little side hustle never hurt anyone) you can think of publishing your Native App to the Marketplace too.</p><h4>Goal</h4><p>I’ve used these QuickStart materials from Snowflake before to get familiar with the foundations of Native Apps, which are a good starting point for anyone starting their Native App development journey:</p><ul><li><a href="https://quickstarts.snowflake.com/guide/getting_started_with_native_apps/index.html?index=..%2F..index#0">Getting Started with Snowflake Native Apps</a></li><li><a href="https://quickstarts.snowflake.com/guide/data_mapping_in_native_apps/index.html?index=..%2F..index#0">Data Mapping in Snowflake Native Apps using Streamlit</a></li><li><a href="https://quickstarts.snowflake.com/guide/native-app-chairlift/index.html?index=..%2F..index#0">Build a Snowflake Native App to Analyze Chairlift Sensor Data</a></li><li><a href="https://www.snowflake.com/snowflake-native-app-bootcamp/">Native App Bootcamp</a></li><li><a href="https://www.snowflake.com/snowflake-native-app-developer-toolkit/">Native App Developer Toolkit</a></li></ul><p>Now, with my April Month Goal, I decided to use that foundation to build something, more specifically to create a Native App from an <em>existing</em> Streamlit application. My goal is to:</p><ul><li>Create a Native App from an <em>existing</em> Streamlit application</li><li>Share that Native App <em>within </em>my Snowflake Organization (and not on the Marketplace)</li></ul><p>So I picked up my dream team’s previous Streamlit creation, <a href="https://medium.com/snowflake/frostygen-a-streamlit-data-generator-tool-f28950b55c41">FrostyGen</a>, and turned it into a Native App so that other users in our Snowflake Sales Engineering Organization can generate synthetic data for demos or POCs too. If you are a Provider of a Native App, you can share your Native Application well beyond your own Organization by creating a Marketplace Listing. For development purposes or for sharing internal tools, you don’t necessarily need to publish your Native App to the Marketplace.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*frLQ5jl757f_G375" /><figcaption><a href="https://medium.com/snowflake/frostygen-a-streamlit-data-generator-tool-f28950b55c41">FrostyGen</a></figcaption></figure><h4>Creating my Native App</h4><p>Below, I’ll walk you through the three main conceptual differences between building an app with Streamlit in Snowflake and Native App with Streamlit UI.</p><ul><li>Create an Application Package with setup.sql and manifest.yml</li><li>Using reference(‘object’)</li><li>Defining privileges for the app_role and consumer</li></ul><h4>1️⃣ <strong>Create an Application Package with Setup.sql and Manifest.yml</strong></h4><p>Next to your streamlit.py file, you’ll need to create at least a setup.sql and manifest.yml file in order to create an Application Package for your Native App.</p><p>The<strong> setup script</strong> contains the creation of an application role, any tables that are used by the application itself (and hidden to the consumer unless explicit access is granted to the consumer), the Streamlit object based on your streamlit.py, procedures such as one to set, remove or clear references to tables by the consumer. Remember that your setup.sql is going to be rerun every time an application is upgraded, so use IF NOT EXISTS and OR REPLACE where needed.</p><p>The <strong>manifest file</strong> contains at least the version of your application, as well as artifacts that are distributed from this version of the package (such as readme, setup script and default streamlit) and runtime configuration for this version. Finally, any references and privileges required are defined in the manifest file too.</p><p>The<strong> Application Package</strong> is a Snowflake object that can be created using SQL or using the Snowsight UI. The package encapsulates all the data content, application logic, metadata and setup script required by your application. In the Application Package object, you will create a Stage in which you store all the necessary materials for your application (but at least the setup.sql and the manifest.yml).</p><h4>2️⃣ <strong>Using reference(‘object’)</strong></h4><p>In a Native App you can use objects that you created in the setup.sql or objects from the consumer in the consumer account. The consumer must give the Native App access to any objects that it requires. This is done using References: in the Native App logic, you’ll use reference(‘object’) and the consumer is asked which object they want to give access to for ‘object’.</p><p>A reference can contain various object types, such as tables, views, but also warehouses and API integrations. The following list describes the object types that a reference can include and the privileges allowed for each object (from <a href="https://docs.snowflake.com/en/developer-guide/native-apps/requesting-refs#object-types-and-privileges-that-a-reference-can-contain">Snowflake Documentation</a>):</p><ul><li>TABLE : SELECT, INSERT, UPDATE, DELETE, TRUNCATE, REFERENCES</li><li>VIEW: SELECT, REFERENCES</li><li>EXTERNAL TABLE: SELECT, REFERENCES</li><li>FUNCTION: USAGE</li><li>PROCEDURE: USAGE</li><li>WAREHOUSE: MODIFY, MONITOR, USAGE, OPERATE</li><li>API INTEGRATION: USAGE</li></ul><p>Snowflake provides an example Stored Procedure for handling a call back for the reference, as can be seen below, which calls a system function to perform a specific operation (add, remove or clear) on a reference that is passed as an argument to the stored procedure which was helpful.</p><pre>CREATE APPLICATION ROLE app_admin;<br><br>CREATE OR ALTER VERSIONED SCHEMA config;<br>GRANT USAGE ON SCHEMA config TO APPLICATION ROLE app_admin;<br><br>CREATE PROCEDURE CONFIG.REGISTER_SINGLE_REFERENCE(ref_name STRING, operation STRING, ref_or_alias STRING)<br>  RETURNS STRING<br>  LANGUAGE SQL<br>  AS $$<br>    BEGIN<br>      CASE (operation)<br>        WHEN &#39;ADD&#39; THEN<br>          SELECT SYSTEM$SET_REFERENCE(:ref_name, :ref_or_alias);<br>        WHEN &#39;REMOVE&#39; THEN<br>          SELECT SYSTEM$REMOVE_REFERENCE(:ref_name);<br>        WHEN &#39;CLEAR&#39; THEN<br>          SELECT SYSTEM$REMOVE_REFERENCE(:ref_name);<br>      ELSE<br>        RETURN &#39;unknown operation: &#39; || operation;<br>      END CASE;<br>      RETURN NULL;<br>    END;<br>  $$;<br><br>GRANT USAGE ON PROCEDURE CONFIG.REGISTER_SINGLE_REFERENCE(STRING, STRING, STRING)<br>  TO APPLICATION ROLE app_admin;</pre><p>For example, in FrostyGen, a user can select a table to save the generated data to. The app cannot determine the name of the schema and object in the consumer account. Using Native App concepts, one can create the reference by calling the SYSTEM$REFERENCE system function and running the callback stored procedure passing the id of the reference.</p><p>The Snowflake Native App Framework provides a Python Permission SDK that allows providers to embed requests for the consumer in the user interface. Snowsight displays the access requests in the <strong>Security</strong> tab of the installed Snowflake Native App.</p><p>With this possibility of using reference(‘object’), you might find that you have to get used to thinking about objects as references to objects of which you don’t know the details during your Native App development.</p><h4>3️⃣ Defining <strong>privileges</strong></h4><p>The last brain trainer for me was to think about all privileges needed in your (Streamlit) application.</p><ul><li>Should the application be able to create objects outside of the Native App in the consumer account?</li><li>Should databases be created in the application itself that are visible to the consumer of the app?</li></ul><p>For example, in FrostyGen I used an intermediary table to store the generated data before it was saved into a consumer table of choice. I wanted that intermediary table visible to the consumer in Snowsight. For this, you can create a table in your application and grant access to the app_role. If the application role is not granted permissions onto an object then you will not see the object in Snowsight. The application itself can still use the objects regardless.</p><p>An application can request global privileges (aside from references to objects) to a consumer. You can request the following global privileges in the consumer account (from <a href="https://docs.snowflake.com/en/developer-guide/native-apps/requesting-privs#privileges-the-provider-can-request-from-the-consumer">Snowflake Documentation</a>):</p><ul><li>EXECUTE TASK</li><li>EXECUTE MANAGED TASK</li><li>CREATE WAREHOUSE</li><li>MANAGE WAREHOUSES</li><li>CREATE DATABASE</li></ul><p>I didn’t use this in FrostyGen but I can imagine using this for creating a separate warehouse to run the data generation scripts, which as an added benefit gives the consumer complete transparency over how many credits running this Native App is consuming.</p><h4>Sharing my Native App</h4><p>Finally, my Native App was coming together, and I had my setup script, manifest and Streamlit file — it’s time to actually create the Application Package. I’m asked to choose a distribution option:</p><ul><li>Distribute to accounts outside your organization</li><li>Distribute to accounts in your organization.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6SxjPHqHLBBLYMbg" /><figcaption>Choosing a Distribution Option for your App Package (from Snowflake UI)</figcaption></figure><p>For my project, I’m selecting Distribute to accounts in your organization, as I’m not intending to share this on the Marketplace. It also means that there’s no required security scan, which would be part of distributing the Native App outside your organization. <br>At this point you can share your Application Package through a listing, sharing it with Only Specified Consumers, or if you’d like to install the Application yourself (in your own provider account), you can install the application like this:</p><pre>— ################################################################<br>— INSTALL THE APP IN THE ACCOUNT<br>— ################################################################<br><br>USE DATABASE NATIVE_APP_DB;<br>USE SCHEMA NATIVE_APP_SCHEMA;<br>USE WAREHOUSE NATIVE_APP_WH;<br><br>— This executes “setup.sql”; This is also what gets executed when installing the app<br><br>CREATE APPLICATION NATIVE_APP FROM application package NATIVE_APP_PACKAGE using version V1 patch 0;</pre><p>You can now see your Native App listed under Apps in the UI. What’s a better reward to my April Challenge than to see my creation come to life like that?</p><h4>Conclusion</h4><p>In summary, the transition from standalone Streamlit apps to Native Apps in Snowflake offers centralized management and distribution across organizational accounts. By encapsulating logic in Application Packages and carefully defining privileges, you can foster collaboration and innovation in data analytics and application development.</p><p>Interested in building apps in Snowflake and going to 2024 Data Cloud Summit? Don’t miss out on these session:</p><ul><li><a href="https://reg.summit.snowflake.com/flow/snowflake/summit24/sessions/page/catalog/session/1708733172345001ywhx">Best Practices: How to Build Enterprise-Grade Snowflake Native Apps</a></li><li><a href="https://reg.summit.snowflake.com/flow/snowflake/summit24/sessions/page/catalog/session/1708733177887001yxkm">Hands-On-Lab: Build a Snowflake Native App with Snowpark Container Services</a></li><li><a href="https://reg.summit.snowflake.com/flow/snowflake/summit24/sessions/page/catalog/session/1708733146522001yB9o">Optimizing Efficiency in Autonomy Zones with the Snowflake Native App Framework (Bayer)</a></li><li>Don’t forget to register for <a href="https://www.snowflake.com/summit/devday/">Snowflake Dev Day</a> on June 6th (Wednesday) too!</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6s7jlgeA2zho38bF.png" /><figcaption>Snowflake Data Cloud Summit 2024 (<a href="https://www.snowflake.com/summit/">https://www.snowflake.com/summit/</a>)</figcaption></figure><p><em>Special thanks to </em><a href="https://www.linkedin.com/in/ranyabellouki/"><em>Ranya Bellouki</em></a><em> (Sales Engineer @ Snowflake) for reviewing before publication!</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9e30fe8bb64c" width="1" height="1" alt=""><hr><p><a href="https://medium.com/snowflake/turning-a-streamlit-in-snowflake-into-a-native-app-my-april-month-goal-9e30fe8bb64c">Turning a Streamlit in Snowflake into a Native App — My April Month Goal</a> was originally published in <a href="https://medium.com/snowflake">Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, &amp; Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The art and science of nested, polymorphic JSON transformation in Snowflake]]></title>
            <link>https://medium.com/snowflake/the-art-and-science-of-nested-polymorphic-json-transformation-in-snowflake-0864478da413?source=rss----34b6daafc07---4</link>
            <guid isPermaLink="false">https://medium.com/p/0864478da413</guid>
            <category><![CDATA[json-flattening]]></category>
            <category><![CDATA[snowflake]]></category>
            <category><![CDATA[data-engineering]]></category>
            <category><![CDATA[polymorphic]]></category>
            <category><![CDATA[native-app]]></category>
            <dc:creator><![CDATA[Jon Farr]]></dc:creator>
            <pubDate>Fri, 26 Apr 2024 19:32:13 GMT</pubDate>
            <atom:updated>2024-04-26T19:32:12.055Z</atom:updated>
            <content:encoded><![CDATA[<h4>How to dynamically transform JSON into actionable insights by leveraging the Snowflake Native App Framework</h4><p><em>Written by Jon Farr, Founder and CEO of TDAA!, in partnership with Andrew Curran, Founder &amp; COO of TDAA!, and Cameron Wasilewsky, Senior Sales Engineer at Snowflake</em></p><p>During my journey into the depths of data, I’ve encountered every kind of data-related challenge you can think of. Often those challenges are specific to the environment, but the frustration of processing nested hierarchical data that can be polymorphic has been one of the most challenging in my entire career. This is one of my industry’s most intricate problems, affecting database management systems (DBMS) and advanced programming paradigms in equal measure. We’ve made solving this problem our business at TDAA! with our product, <a href="https://www.datapancake.com">Pancake</a>. My hope is that I can effectively communicate my passion for revealing the complexities hidden in these data structures based on the decades I’ve spent focused on information and data architecture.</p><h3>The Problem</h3><p>Our adventure began when I started working with my new co-founder Andrew, and we set out to address the issue of hallucinations in gen AI by focusing on solutions designed to improve the quality of data made available to an LLM. As part of this process, we chose to work with the FDA’s Medical Device data, a free dataset provided by the US government (a dataset that Paul Horan has documented working within <a href="https://medium.com/snowflake/working-with-large-json-files-in-snowflake-part-iv-now-in-python-e6e9ecafc7ff">his excellent blog series</a> on working with large JSONs in Snowflake).</p><blockquote>In working with government data and a very well-defined data dictionary the last thing I expected to encounter was polymorphic data. While flattening the nested JSON structure I discovered that some of the array attributes could contain primitive values such as string or could contain full objects with nested arrays. This was a complex problem that needed a more dynamic and eloquent solution.</blockquote><p>I have always appreciated the flexibility and advantages of hierarchical data formats like JSON , but I am also very aware of the technical debt they can bring to application and data warehouse databases. That flexibility can inadvertently introduce layers of complexity, particularly when it involves issues like polymorphic attributes or nested hierarchical data structures.</p><p>Manually parsing a large, deeply nested, polymorphic JSON is incredibly cumbersome. It is tedious, complicated, and error-prone, but this kind of data is pivotal for analytics and product development. Unfortunately, the challenges of polymorphic forms and the implications of schema evolution often make the opportunity cost of activating this data too high.</p><p>The ubiquity of hierarchical data as a storage format and the likelihood of a given data source containing these complex issues is precisely what we’ve aimed to simplify with Pancake.</p><blockquote>Our goal is to unlock the value of this kind of data by transforming it into a clearly defined structure with visibility into all attributes at every level, regardless of the data types used for each attribute over time.</blockquote><p>We are thinking of the many engineers and analysts who are negatively impacted by a lack of access to data. The time it takes to unpack this data, or deal with its polymorphic nature, means that they most likely do not have access to all of the data. This problem impacts application development as well because attributes in a document database frequently change — but if data migrations are not done successfully, the quality of the application can be impacted tremendously.</p><h3>Technical Deep Dive</h3><p>Our architectural blueprint was centered on thoroughly analyzing a data source containing schema-less, semi-structured JSON data, including its nested hierarchy and the polymorphic state of every attribute. We developed a system that identifies and adapts to the data’s evolving structure, ensuring our output remains accurate and relevant. This system, integrated with the <a href="https://docs.snowflake.com/en/developer-guide/native-apps/native-apps-about">Snowflake Native App Framework</a>, will operate seamlessly within the customer’s environment, maintaining data integrity and security.</p><p>We follow four steps to process any dynamic complex JSON structure, enabling the extracting and flattening of that data into a relational table format.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/692/0*arcFOCPC8d2N4N7U" /><figcaption>Pancake — Native App Architecture</figcaption></figure><h3>Step 1 — Scan a data source containing JSON objects and discover the schema and any polymorphic attributes</h3><p>Our application scans a Snowflake variant column, which has rows of JSON objects to discover every attribute,by recursively traversing the entire object in each row to tackle the intricacies of schema-less, hierarchical data. During this discovery phase, our app generates detailed metadata for each attribute, accommodating any polymorphism observed in such data.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*nVfdTTgnrdl3-D7X" /><figcaption>Source JSON Data with nested object arrays and polymorphic attributes</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*_VLR-j5J-M_hlc4N" /><figcaption>Pancake — Customer data sources that have been scanned and analyzed</figcaption></figure><h3>Step 2 — User configuration of metadata</h3><p>Users are empowered to configure some of the metadata generated in Step 1, including optionally overriding the inferred Snowflake data type, precision, scale, and datetime format. For all array attributes, the user will be able to specify the natural foreign keys by selecting parent-level attributes. These natural foreign keys will be used to create relational dynamic tables.</p><p>The system recognizes and adapts to the various data types an attribute might embody. It intelligently infers the appropriate Snowflake data type for each piece of data, drawing on a sample anonymized value. This process includes discerning a date or datetime data type from a string based on varied ISO standard formats.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rzrF2NHcutjZx9rZ" /><figcaption>Customer data source metadata that can be configured to generated Dynamic Table creation SQL statements</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9bE2TznHLLoYwq8A" /><figcaption>Pancake view of attributes discovered during the scanning process</figcaption></figure><h3>Step 3 — Generate SQL to create nested Snowflake Dynamic Tables and a consolidated view of the JSON schema all polymorphic versions for each attribute</h3><p>Our tool uses the generated and user-configured metadata from Steps 1 and 2 to generate SQL statements to establish a series of nested Snowflake Dynamic Tables. These Dynamic Tables serve as the structured, accessible outcome of what was once a tangled web of nested and polymorphic data. Each polymorphic attribute will be defined in the Dynamic Table as one or more columns based on each data type used by the attribute.</p><blockquote>Users can then create select statements by joining these dynamic tables at various levels by using the natural foreign key attributes created during the configuration process, enhancing downstream data transformation, interconnectivity, and analysis.</blockquote><p>Our app will also generate a consolidated view of the discovered JSON schema and all polymorphic versions of an attribute with their associated data type.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/775/0*KiA9FuhvTyIMm_gG" /><figcaption>Dynamic tables created from the SQL code generation provided by Pancake to extract and flatten complex JSON data</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fmh3E2JGGkojAQSF" /><figcaption>A Snowflake graph view representing the dynamic tables created by Pancake</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*cIxf6_pvkT-sWxQt" /><figcaption>A Snowflake graph view representing the dynamic tables created by Pancake</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*dXMa_mRETx_z9XMT" /><figcaption>A Snowflake graph view representing the dynamic tables created by Pancake</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/956/0*-K6BvKprTdmFlVdf" /><figcaption>Select statement using the dynamic tables created by Pancake</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9GbhcFyzNvafNiOY" /><figcaption>Output of a consolidated JSON document showing all instances of polymorphic attributes from the source dataset</figcaption></figure><h3>Step 4 — Post-flattening: Monitor and maintain</h3><p>A monitoring feature is integral to our app. It ensures that the system remains current with the data it processes. It alerts users to schema modifications in the source data, prompting them to initiate the Dynamic Table SQL generation process. Once generated, the user can deploy the newly updated SQL to update the Dynamic Tables to reflect the latest data structure, maintaining the accuracy and relevance of the transformed data.</p><h3>Our game changers</h3><p><strong>Developing SQL code to create Snowflake Dynamic Tables dynamically</strong></p><p>Our app revolutionizes JSON data parsing, extracting and flattening by generating SQL code to produce Snowflake Dynamic Tables that adapt dynamically to the ever-changing structure of nested data, including the support for polymorphism at the attribute or object level. This approach streamlines the transformation process and ensures that the data remains accurate and reliable, irrespective of its evolving nature.</p><p>Each attribute can take the shape of three major types of data: primitives (string, decimal, bool, etc), arrays (primitive or object), and objects (which can contain additional nested objects or arrays).</p><blockquote>Our app generates Dynamic Table create statements with column definitions to support each type of data an individual attribute has ever used.</blockquote><p>So if one attribute takes the shape of four different data types, then one column will be defined for each data type contained in a single attribute. In our example, you will see address_str and address_array_object. This allows data engineers and analysts to begin working with data immediately and clearly understand the shape of the data. Additional transformations are much simpler with these types of column definitions.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/454/0*38QefGEqwt-ZTWW1" /><figcaption>Select statement from one of the dynamic tables created by Pancake showing the selection of data from a primitive version of the address attribute</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/626/0*hzn4N3Ea093h-C2A" /><figcaption>Select statement from one of the dynamic tables created by Pancake showing the selection of data from an array version of the address attribute</figcaption></figure><p><strong>Monitoring and automation</strong></p><p>Pancake has the ability to monitor data sources and track changes for each configured data source. This enables the additional user configuration of any detected changes so the user can then regenerate the SQL code necessary to update the Dynamic Table definitions through a versioning system. The monitoring capability eliminates issues caused when teams are not alerted to upstream system data schema changes, ensuring the data processing remains uninterrupted and accurate, which enhances operational efficiency.</p><blockquote>By monitoring the raw data source through a Snowflake stream, Pancake can report any data changes to the raw data sources, allowing for additional user configuration and SQL code generation. You will be able to set up monitoring and generation for multiple environments such as dev, test, staging, production, etc.</blockquote><p><strong>Performance optimization</strong></p><p>We are also working several performance optimization strategies to address the challenge of processing large volumes of complex data. Leveraging Snowflake’s advanced features, the team will ensure that Pancake discovers data with minimal latency, maximizing efficiency and providing a seamless user experience.</p><h3>Why Snowflake?</h3><p>Leveraging Snowflake as our platform is a strategic decision. The robust capabilities for managing semi-structured data offered the perfect foundation for Pancake. It leverages Snowflake’s variant data type, enabling us to dynamically identify and catalog every attribute within the nested JSON and adapt our approach to address the data’s polymorphic nature. Snowflake’s new Dynamic Tables allow us to create streams that extract and flatten the hierarchical and polymorphic data into clearly defined structures that bring accurate visibility into every possible shape the data source may represent. These streams enable accurate and complete use for further transformations or analysis.</p><blockquote>Further, the Snowflake Native App Framework and Snowflake Marketplace simplify our go-to-market approach; we do not need to worry about distribution, security, or licensing.</blockquote><p>The other thing that’s been really helpful during our development is all of the support that we’re getting through the<a href="https://investors.snowflake.com/news/news-details/2023/Snowflake-Launches-Powered-By-Snowflake-Funding-Program-Investing-Up-to-100-Million-in-Innovative-Apps-in-the-Data-Cloud/"> Powered by Snowflake Funding Program</a>. As we’ve been building out the product, we’ve had biweekly meetings with their technical team as well as a Slack channel where we can ask ad hoc questions. This has been really helpful during our design and build phase. As we get into testing and launching, the program has also helped us find potential design partners and customers through direct introductions, co-selling motions, and co-marketing opportunities.</p><p>Our story with Snowflake is more than just about building an application; it’s a narrative of innovation, a testament to the power of technical expertise combined with a clear vision and unwavering commitment to solving the puzzles presented by complex data.</p><h3>What’s next?</h3><p>Pancake is now open to beta customers, so please reach out if you are interested in previewing our new Native Snowflake App. The Snowflake Native App, which performs the scan and discovery parts of the process, will launch in the weeks ahead of the <a href="https://www.snowflake.com/summit/?utm_cta=website-homepage-premium-summit-agenda-builder&amp;utm_campaign=gl-ob-al-ly&amp;utm_source=google&amp;utm_term=c-g-snowflake-data-summit-e-692483594627&amp;utm_medium=paidsearch&amp;utm_content=go-sitelink-evp-evp-summit2024&amp;gclid=CjwKCAjwoa2xBhACEiwA1sb1BDBAhhygJ6YWZkaeiI-Abu86OV9LTWfbmgVnDU6pa2GFjJWX8eHvdRoCfS0QAvD_BwE">2024 Snowflake Data Cloud Summit</a> on June 3rd. At that point, you’ll be able to download it into your Snowflake environment and scan your complex JSON data to retrieve a free analysis of the polymorphic schema and complexity — or upgrade to perform in-depth schema analysis on those same sources. Following that, we’ll release our external app, which will allow for additional configuration and Dynamic Table create statement generation. Watch this space for more on that topic in a future post, and stay tuned for more insights into TDAA!’s technical odyssey and Snowflake Native App. In the meantime, reach out; we will happily chat. You can also contact us directly at <a href="mailto:jon@tdaa.ai">jon@tdaa.ai</a> or <a href="mailto:andrew@tdaa.ai">andrew@tdaa.ai</a> or you can learn more at <a href="http://www.datapancake.com/">www.datapancake.com</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=0864478da413" width="1" height="1" alt=""><hr><p><a href="https://medium.com/snowflake/the-art-and-science-of-nested-polymorphic-json-transformation-in-snowflake-0864478da413">The art and science of nested, polymorphic JSON transformation in Snowflake</a> was originally published in <a href="https://medium.com/snowflake">Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, &amp; Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Snowflake Arctic Cookbook Series: Arctic’s Approach to Data]]></title>
            <link>https://medium.com/snowflake/snowflake-arctic-cookbook-series-arctics-approach-to-data-b81a8a0958bd?source=rss----34b6daafc07---4</link>
            <guid isPermaLink="false">https://medium.com/p/b81a8a0958bd</guid>
            <category><![CDATA[arctic-llm]]></category>
            <category><![CDATA[large-language-models]]></category>
            <category><![CDATA[snowflake-arctic]]></category>
            <category><![CDATA[llm-training]]></category>
            <dc:creator><![CDATA[Snowflake AI Research]]></dc:creator>
            <pubDate>Fri, 26 Apr 2024 14:46:15 GMT</pubDate>
            <atom:updated>2024-04-26T14:46:14.995Z</atom:updated>
            <content:encoded><![CDATA[<p>On April 24, we released <a href="https://www.snowflake.com/blog/arctic-open-and-efficient-foundation-language-models-snowflake">Snowflake Arctic</a> with a key goal in mind — to be truly open. In line with that goal, the Snowflake AI Research team is writing a series of cookbooks to describe how to pretrain, fine-tune, evaluate, and serve large-scale MoEs such as Arctic. We will share our journey of training the Arctic model, our findings related to sourcing and composing pre-training data, designing MoE architectures, co-designing models with training and inference systems in mind, and methods for fine-tuning and evaluating the models.</p><blockquote>For the full series, please go to <a href="http://snowflake.com/en/data-cloud/arctic/cookbook"><strong>Snowflake Arctic cookbook catalog</strong></a><strong>.</strong></blockquote><h3>Data is the cornerstone of high-quality Large Language Models (LLMs). It is the fuel that powers the intricate engine of an LLM, enabling it to learn, adapt, and evolve.</h3><p>Doing a great job with the data recipe for LLMs is full of hard challenges:</p><ul><li><strong>The Voracious Token Appetite of LLMs</strong>: The modern LLM training stack is insatiable, requiring several trillions of tokens. These tokens need to be high-quality, domain-specific, and diverse. To put this in perspective, one trillion tokens is roughly equivalent to the content of 15 million books. Simply repeating tokens to meet this colossal requirement can lead to <a href="https://arxiv.org/pdf/2305.13230">performance degradation</a>. The pressing question remains: where can we source such an immense volume of data?</li><li><strong>The Quest for Quality in Enterprise Data</strong>: For the enterprise tasks that Arctic zeroes in on, high-quality code and SQL data are paramount. Assembling a corpus of high quality tokens for these data sources is not trivial.</li><li><strong>Processing Massive Raw Data</strong>: Once a research team has a large corpus of tokens at hand, the challenge shifts to processing it effectively and scalably.</li><li><strong>Deciphering Data Composition and Curriculum</strong>: Understanding the makeup of our data and structuring a learning curriculum for LLMs is a complex puzzle to solve.</li></ul><p>Arctic is trained on 3.5 trillion tokens sourced from the public domain, encompassing web content, code &amp; SQL, STEM, and more.</p><p>In this blog, we go into the origins of our data sources and the methodologies employed to elevate them to the desired quality. We provide an overview of the approaches we’ve taken to tackle the first three challenges head-on. We describe our strategies for 1) assembling vast quantities of web data, 2) gathering high quality enterprise-focused datasets, and 3) data processing techniques and pipeline enhancements to refine data quality. By sharing an insider’s view of the data sources, techniques, and configurations that have proven successful for us, we aim to provide valuable insights to our readers.</p><p>At the end, we offer a sneak peek on how we’ll address the 4th challenge about data composition and curriculum, which we will dive into in an upcoming blog on Arctic data.</p><h3>Assembling High Quality Web Data</h3><h4>Start with high precision web crawl data as base</h4><p>Web crawls are a great starting place to pre-train an LLM. We started with high quality data sources that have been extensively used in the research literature: <a href="https://huggingface.co/datasets/allenai/c4">C4</a> (originally described in the <a href="https://arxiv.org/abs/1910.10683">T5 paper</a>) &amp; <a href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb">Refined Web</a> (<a href="https://arxiv.org/abs/2306.01116">used to train the Falcon LLM</a>). These two data sources are pre-processed from the <a href="https://commoncrawl.org/">Common Crawl dataset</a> to improve performance by throwing out low quality text. These datasets already have language filtering, deduplication and quality filtering. In addition, we performed further document quality based filtering based on the KenLM perplexity score (using fast n-gram <a href="https://huggingface.co/edugp/kenlm">models</a> trained on Wikipedia, and the one that is not similar to Wikipedia will have a high perplexity score). We filtered any doc with a score greater than 1000. This roughly reduced the number of documents by 20%, but increased our commonsense reasoning in ablation by ~2%. These two data sources combined gave us ~750B tokens.</p><h4>Where do we get high recall web crawl data?</h4><p>Merely using high precision web crawl data is not sufficient to provide the volume of tokens we want. To add more, we used an annotated set of 84 different Common Crawl (CC) web dumps from Together.AI. This dataset, while gigantic, is not of the same quality as RefinedWeb and C4 on average. At the same time, it does have annotations including perplexity score and over 40 useful signals such as “head”, “middle”, and computed minhashes. This leads us to our next question.</p><h4>How do we filter the high recall web data to match the high precision web data in quality?</h4><p>We ran several dataset ablations to come up with a good filtering criteria over the <em>high recall</em> web crawl dataset. Holding the high precision data as the gold standard, we trained an MoE model on 27B tokens on this data. Let’s call this model <em>high-precision-MoE</em>.</p><p>For every choice of filtering criteria, we generated 27B tokens from random documents from the high recall web dataset that met the filtering criteria and trained another MoE model on this data. Call this <em>filter-high-recall-candidate-MoE</em>. We then hill-climbed on the filtering criteria until we got the performance of filter-candidate-MoE to match that of the high-quality-MoE. As our MoE architecture for ablations, we used a nimble 350M x 64 expert model and for comparisons, we used the stable eval harness (average of 9 common sense metrics). Here is our exact filtering config:</p><pre>See the Together.AI V2 web dataset description for variables definition.<br><br>rps_doc_word_count &lt; 50<br>rps_doc_word_count &gt; 100000<br>rps_doc_mean_word_length &lt; 3<br>rps_doc_mean_word_length &gt; 10<br>rps_doc_symbol_to_word_ratio &gt; 0.1<br>rps_doc_frac_lines_end_with_ellipsis &gt; 0.3<br>rps_doc_frac_no_alph_words &gt; 0.2<br>ccnet_perplexity &gt; 1000000<br>rps_doc_frac_chars_dupe_10grams &gt; 0.1<br>rps_doc_frac_chars_dupe_9grams &gt; 0.11<br>rps_doc_frac_chars_dupe_8grams &gt; 0.12<br>rps_doc_frac_chars_dupe_7grams &gt; 0.13<br>rps_doc_frac_chars_dupe_6grams &gt; 0.14<br>rps_doc_frac_chars_dupe_5grams &gt; 0.15<br>rps_doc_frac_chars_top_2gram &gt; 0.2<br>rps_doc_frac_chars_top_3gram &gt; 0.18<br>rps_doc_frac_chars_top_4gram &gt; 0.16<br><br># The following need to be calculated indirectly from the raw signals.<br># If the url is in the ut1 blacklist<br>ut1_blacklist = true<br># If there are no stopwords<br>num_stopwords = 0<br># If the fraction of number of lines that start with a bullet is greater than 0.9<br>max_percent_lines_start_with_bullet &gt; 0.9  <br># If the fraction of number of lines that have at most 1 word is greater than 0.05<br>max_percent_lines_min_num_words &gt; 0.05<br># If the fraction of number of lines that are purely numeric are is greater than 0.05<br>max_percent_lines_purely_numeric &gt; 0.05<br># If the fraction of number of lines that contains only upper case characters is greater than 0.05<br>max_percent_lines_too_uppercase &gt; 0.05</pre><p>Even with a reasonable filtering config, there was still a puzzling quality gap between the high precision web data and the high recall web data (even after filtering the latter dataset). We found that this gap was because we were always using the most recent Common Crawl when doing ablations. We removed this filter and pulled from CommonCrawl going back 10 years and — <em>voila</em>! — we matched (actually slightly exceeded) the high precision dataset’s performance. Our hypothesis for why this is happening is that Common Crawl is blocked more in recent years and it’s possible that crawl quality is declining over time.</p><p>Following this work, we were able to extract around 2.5T tokens from the web to use in pre-training.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ga6bmU0J0BDw56vd" /><figcaption><strong>Figure 1: Quality improvements measured by CommonSense from different filtering techniques over the high recall web data</strong></figcaption></figure><h3>Data for Enterprise Tasks</h3><p>At Snowflake, we see a consistent pattern in AI needs and use cases from our enterprise customers. Enterprises want to use LLMs to build conversational SQL data copilots, code copilots, and RAG chatbots. From a metrics perspective, this translates to LLMs that excel at SQL, code, complex instruction following, and the ability to produce grounded answers.</p><p>To start with making our model better at enterprise tasks, we focused on pulling more SQL, programming and math data.</p><h3>Programming Data</h3><p>Since we wanted our model to be good at complex programming tasks, it was important to train with a large amount of high quality coding data. We assembled our code dataset from several sources including <a href="https://huggingface.co/datasets/bigcode/starcoderdata">StarCoder</a>, the public <a href="http://cloud.google.com/bigquery/public-data/github">GitHub Dataset</a>, and PyPi. We used the GitHub API to collect a list of all repositories with at least 10 stars to ensure a minimum quality standard.</p><h4>Add file-level attributes to make data easily filterable</h4><p>Following StarCoder, we added attributes at <em>file level</em> for the Github data we extracted to make the data filterable for quality. We used the Snowflake cloud data warehouse to process our data with one table for code metadata (filenames, stars, license) and another for the content itself keyed by content hash. We added columns to our metadata table for any attribute we might want to use for filtering or sorting. Once the dataset was collected, we labeled each file’s metadata with the same filterable attributes that StarCoder uses like noisy_html (if there is too much boilerplate in a .html file), percent_alphanum (what percentage of the file is alpha-numeric characters) and pct_html_visible (what percentage of the HTML document would be visible if rendered). These attributes came in handy to create the highest quality data.</p><h4>Add repo-level attributes to identify duplicate repositories</h4><p>In addition to file-level attributes, we labeled repositories with whether or not we believed they were duplicates. We used the GitHub fork bit wherever available. Unfortunately, it’s surprisingly common for repositories to have been forked via git clone instead of the fork UI in GitHub, so the GitHub fork signal is not always present in the Github API response. To solve this, we also deduced whether or not two repositories were duplicates (or at least very similar) by their content overlaps and by any shared commit history.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/477/1*Gxz3rnVEnwFNwKX-qUQ4Lg.png" /><figcaption>List of attributes collected and used in filtering.</figcaption></figure><h4>Decontamination, and all that good stuff</h4><p>We followed the <a href="https://arxiv.org/abs/2401.14196">DeepSeek Coder</a> recipe in its method for decontamination. We annotated all content in our code repository with an is_contaminated bit if it has textual overlap with any of our evaluation data. There are a surprisingly large number of <a href="https://paperswithcode.com/dataset/mmlu">MMLU</a> snippets floating around on GitHub! It’s very easy for model trainers to accidentally train their model on the same data they later evaluate on.</p><h4>Topological sorting</h4><p>After generating data, we followed the technique pioneered by DeepSeek-Coder, where we topologically sort all the files within a repository in the order in which they are imported. To do this, we first broke repositories up by their constituent programming languages. Then we constructed a dependency graph of which file imports which. We then traversed this graph to produce a single, large document per repository and language where the files included are closer to the top of the file. Whenever there were ties, we ordered the files lexicographically, but taking into account their directory depth. Finally, we had directional indication from experiments that led to us also adding the data in its original, ungrouped form into our data recipe.</p><h4>Tokenization and ablations</h4><p>Each of these filter criteria we have mentioned gets added to our code metadata database tables. We then wrote a Snowflake query to select data with various quality and software license filters per programming language.</p><p>At this point we can finally tokenize these documents for pre-training. At the end of the process, we have a cleaned, topologically sorted, and tokenized dataset per programming language. This allows us to specify what exact data mixture of tokens we want not just over code, but over each programming language in our dataset.</p><h3>Coding, SQL, and Math topics from Web Corpus</h3><p>To improve our model quality in math and coding (Python and SQL in particular), we selected subsets of web crawl data closely relevant to those topics: tutorials, blog posts, documentation and textbooks.</p><h4>Surfacing high quality coding domains and URLs</h4><p>Processing terabytes of webdata and doing fuzzy matches for filtering data is a hard problem. Luckily for us, we work at the best enterprise data cloud company in the world. Once terabytes of web data and metadata were ingested into Snowflake, it was easy to surface the relevant web pages using Snowflake SQL. For example, we were able to source coding data through simple queries:</p><ol><li>looking at URLs — e.g., does a URL contain “python”?</li><li>looking at other signals and metadata — e.g., was it linked with “Python” in the title?</li></ol><p>We went one step further and indexed the web-data to allow us to search the web content itself. This meant we could search for web pages containing such “Python programming”.</p><p>For each topic at hand, we used an expansion process to generate a long list of URLs relevant to the topic. We started by establishing a base set of high precision URLs containing content relevant to the topic at hand (e.g “math”). Then, we augmented this list by calculating the “lift” (or popularity score) for each domain related to our topic using Snowflake SQL. This was achieved by determining the frequency of each domain per topic. If a domain’s lift surpassed a particular threshold, we included all pages from that domain into our set. Once we had our expanded set of URLs, we employed straightforward quality filters, using SQL again, to eliminate any low-quality web pages.</p><p>An interesting sidebar: to get the actual content from a HTML web page, you can use a Python open-source library, for example Beautiful Soup or <a href="https://pypi.org/project/boilerpy3/">boilerpy3</a> (the former is available in <a href="https://repo.anaconda.com/pkgs/snowflake/">our Anaconda channel</a>, whereas the latter can be imported via a stage) embedded as a UDF (<a href="https://docs.snowflake.com/en/developer-guide/udf/udf-overview">user-defined function</a>) in a Snowflake SQL statement. This makes it fast and easy to transform your data all in Snowflake.</p><h3>Other high quality datasets</h3><p>Additionally, we added a few other sources of data that we found very useful for some targeted capabilities during the pre-training phase.</p><ul><li><a href="https://docs.snowflake.com/">Snowflake documentation</a>: For knowledge about Snowflake capabilities</li><li><a href="https://huggingface.co/datasets/open-web-math/open-web-math">Open-Web-Math</a>: Good STEM knowledge</li><li><a href="https://huggingface.co/datasets/HuggingFaceTB/cosmopedia">Cosmopedia</a>: Good source of synthetic datasets</li><li><a href="https://huggingface.co/datasets/Skylion007/openwebtext">Openwebtext</a>: Diverse set of high quality web data, reproducing the WebText dataset used by the <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2 paper</a></li></ul><h3>Data Processing Techniques</h3><h4>How did we filter the data to ensure it was high quality?</h4><p>It is important for downstream performance to filter the data to ensure it is high quality and safe. We focused on filtering for deduplication, removing gibberish, and pornographic content.</p><p>We label a document as redundant if the content of the document is very repetitive. To calculate whether a document is redundant, we count the frequency of n-grams. If an n-gram’s count exceeds a minimum value, we consider that document too redundant. For example, if a 6-gram “data is still loading from source” made up 70% of a document, we remove the document.</p><p>We can perform similar word statistics to throw out docs with garbage text by observing the word length, number of ellipses, and number of bullets. In addition to word statistics, we also used the Wikipedia-based <a href="https://github.com/kpu/kenlm">KenLM</a> model to filter out very high perplexity text. We list the full filters we used in the section on web data.</p><p>For pornographic text, we used a blocklist of words and a separate blocklist of URLs to ignore. All of these filtering heuristics were inspired by RefinedWeb, C4, and the <a href="https://arxiv.org/pdf/2112.11446.pdf">Gopher</a> paper.</p><h4>How did we deduplicate across documents?</h4><p>We employed a MinHash + <a href="https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/">Local Sensitivity Hashing</a> (LSH) pipeline to dedupe across documents. In particular, we followed the approach from the literature which reduces the quadratic problem of comparing all pairs of minhashes to a linear look-up via connected components. To do this, each minhash was partitioned into a specific number of parts (call them sub-hashes), and each document which shared a sub-hash with another was considered a duplicate and part of the same connected component. We determined the number of sub-hashes for each minhash by optimizing the false positive and false negative rate on a holdout set with the duplication information pre-calculated, which resulted in 14 subhashes as our default.</p><p>If done without proper optimization, the total size of the minhashes exceeds 10TB for some of our datasets, which would not fit into memory. We leveraged (again) the data processing capabilities of Snowflake to address this. To perform a global fuzzy connected components problem in Snowflake, we first imported the minhashes and created a table where each row consisted of a document and a minhash partition. We then performed a window function to keep only one instance per subhash. Note that this doesn’t actually compute connected components, as one document can have multiple edges. However, we made a “best effort” to fix this — we first removed all min clusters which were connected to another smaller min cluster. While this approach fails for potential clusters which have nodes with a distance of greater than 2 and are not covered by the other removal strategies, we measured this to be an insignificant percentage of docs that should be removed (&lt;1%).</p><h3>Preview of Arctic Data Composition</h3><p>The final question we need to address is how we compose these datasets into a pre-training dataset and schedule. Arctic was trained with a three stage curriculum each with different data composition focusing on generic skills in the first phase (1T Tokens), and enterprise focused skills on the latter two phases (1.5T and 1T tokens). A high-level summary of our dynamic curriculum is shown in Table 2. In a follow-up blog we will deep dive into a review of the data composition of our model, what techniques worked and equally importantly, what didn’t.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*-9j09CUlZd8_XZ79" /><figcaption><strong>Table 1. Dynamic data composition for three-phase training of Arctic with emphasis on Enterprise Intelligence.</strong></figcaption></figure><p><strong>Learn more in our Snowflake Arctic series</strong></p><p>Check out our other blog posts that dive into Snowflake Arctic training, including data cleaning, training system design, modeling and system design for optimal throughput, etc. Stay tuned as more updates continue to drop in the<a href="http://snowflake.com/en/data-cloud/arctic/cookbook"> Snowflake Arctic cookbook catalog</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b81a8a0958bd" width="1" height="1" alt=""><hr><p><a href="https://medium.com/snowflake/snowflake-arctic-cookbook-series-arctics-approach-to-data-b81a8a0958bd">Snowflake Arctic Cookbook Series: Arctic’s Approach to Data</a> was originally published in <a href="https://medium.com/snowflake">Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, &amp; Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[SQL & Snowflake- New capabilities]]></title>
            <link>https://medium.com/snowflake/sql-snowflake-new-capabilities-9fa28c64384b?source=rss----34b6daafc07---4</link>
            <guid isPermaLink="false">https://medium.com/p/9fa28c64384b</guid>
            <category><![CDATA[snowflake]]></category>
            <category><![CDATA[data-and-analytics]]></category>
            <category><![CDATA[sql]]></category>
            <category><![CDATA[snowflake-data-cloud]]></category>
            <category><![CDATA[data-superhero]]></category>
            <dc:creator><![CDATA[Somen Swain]]></dc:creator>
            <pubDate>Fri, 26 Apr 2024 04:21:40 GMT</pubDate>
            <atom:updated>2024-04-26T04:21:40.002Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/887/1*G3-L_nH83ViSnds3ZJ0bdg.png" /><figcaption>SQL &amp; Snowflake</figcaption></figure><p>SQL has been one of the most powerful language used a across almost all the teams. And this language &amp; its options to use has evolved over period of time. Snowflake is also ensuring that it makes the user experience while using SQL seamless by continuously improving and adding some new capabilities on how this can be used to enable the data workloads &amp; also to improve the productivity.</p><p>In this blog, I would try to capture few of the important and latest announcements on SQL capabilities on Snowflake.</p><ol><li>“SELECT *” with “EXCLUDE”</li><li>“SELECT *” with “RENAME”</li><li>REPLACE</li><li>MIN_BY &amp; MAX_BY aggregate functions.</li><li>GROUP BY ALL</li><li>ILIKE</li><li>IF NOT EXISTS, IF EXISTS</li><li>ARRAY_SORT, ARRAY_MIN, ARRAY_MAX.</li><li>BANKER’s ROUNDING</li><li>“SELECT *” alternative “TABLE”</li></ol><h4>1. SELECT * with “EXCLUDE”:</h4><p>When creating SQL, it’s usual practice to pick every column in a table except for a “select handful”. Naming each column in the SELECT clause is a typical workaround to accomplish this, but it can be time-consuming when there are a lot of columns. This is exactly where “EXCLUDE” keyword along-with SELECT can help :</p><pre>---The below query gives all the columns--<br>SELECT * FROM demo_db.demo_schema.customer_01 LIMIT 10;<br><br>--The below query gives all columns except &quot;C_CURRENT_CDEMO_SK&quot; --<br>SELECT * EXCLUDE C_CURRENT_CDEMO_SK FROM demo_db.demo_schema.customer_01 LIMIT 10;<br><br>--The below query gives all columns except &quot;C_CURRENT_CDEMO_SK&quot; &amp; &quot;C_CURRENT_ADDR_SK&quot; -- <br>SELECT * EXCLUDE (C_CURRENT_CDEMO_SK,C_CURRENT_ADDR_SK)  FROM demo_db.demo_schema.customer_01 LIMIT 10;<br><br><br>Hence the last 2 are the examples of how EXCLUDE can be used in day to day <br>usage of SQL constructs.</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/947/1*D-LWbno7y-yVla8RaQFMcQ.png" /><figcaption>EXCLUDE-SQL keyword</figcaption></figure><h4>2. SELECT * with “RENAME”:</h4><p>We can also rename individual columns in our SELECT statement using Snowflake. Consider it the older sibling of the previously discussed exclude feature; it operates similarly and the example is given as below:</p><pre><br>--Renaming one column in the select statements.<br>SELECT * RENAME C_CUSTOMER_ID AS CUST_ID FROM demo_db.demo_schema.customer_01 limit 10;<br><br>--Renaming more than one columns in select statements.<br>SELECT * RENAME (C_CUSTOMER_ID AS CUST_ID, C_CURRENT_CDEMO_SK AS CDEMO_SK) FROM demo_db.demo_schema.customer_01 limit 10;<br><br><br>--Combining the RENAME &amp; EXCLUDE keywords:<br>SELECT * EXCLUDE C_CUSTOMER_ID RENAME (C_CURRENT_CDEMO_SK AS CDEMO_SK) FROM demo_db.demo_schema.customer_01 limit 10;<br></pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/945/1*V6nKyDu9Gb9DroLuFik3Jg.png" /><figcaption>RENAME- Use cases.</figcaption></figure><h4>3. REPLACE:</h4><p>Another useful SQL feature in our Snowflake is called <strong><em>REPLACE</em></strong>, that allows us to easily change the values of certain columns within our statement. This is useful when we want to specifically adjust the values of one or more columns. Examples are given as below:</p><pre>--Below query changes the values of C_FIRST_NAME to upper cases.<br>SELECT * REPLACE (UPPER(C_FIRST_NAME) AS C_FIRST_NAME) FROM demo_db.demo_schema.customer_01 LIMIT 10;<br><br>--Adjusting multiple column values using REPLACE keyword.<br>SELECT * REPLACE (UPPER(C_FIRST_NAME) AS C_FIRST_NAME, UPPER(C_LAST_NAME) AS C_LAST_NAME ) FROM demo_db.demo_schema.customer_01 LIMIT 10;</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*3FRYckytM45Hpls9KblpHg.png" /><figcaption>Another use case of “REPLACE”</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/949/1*VYCRqLuJzT1UMwAiiuJcHg.png" /><figcaption>REPLACE-&gt; Use cases</figcaption></figure><h4>4. MIN_BY &amp; MAX_BY:</h4><p>These are the functions which allows to find the rows containing the minimum or maximum value of the column and returns the value of another column in that row. With this queries like SELF-JOINs or even SUB-Query can be avoided. Let us see the example, and please note over here we would see the comparison of traditional &amp; modern way of writing the query.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*09iVlFEwq9P_3cODIPVq8Q.png" /><figcaption>Sample Data</figcaption></figure><p>If above is my data set, then if I am supposed to extract against each ORDER_ID what would be my latest status of the order, then how the query would look like traditionally &amp; with modern method. We would see how simpler it is to get the values.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*aWjm_avC0uySjC4VRZ41hA.png" /><figcaption>The USAGE of “MAX_BY” function.</figcaption></figure><pre>--Below is how they are used.<br>SELECT ORDER_ID, MAX_BY(STATUS, ORDER_DT) FROM DEMO_MINBYMAXBY_01 GROUP BY ORDER_ID;<br>SELECT ORDER_ID, MIN_BY(STATUS, ORDER_DT) FROM DEMO_MINBYMAXBY_01 GROUP BY ORDER_ID;</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/993/1*wQLOoInVZyYG7oyOWSUBOA.png" /><figcaption>Some use cases.</figcaption></figure><h4>5. GROUP BY ALL:</h4><p>In most of the cases where there is a need to do aggregation on multiple columns we tend to write GROUP BY with clauses like “GROUP BY C1, C2, C3, etc..” OR “GROUP BY 1,2,3,4”. Few of the downside with this approach is think of it like if we want to “add” OR “remove” some of the columns in the SELECT clause then our SQL query would FAIL.</p><p>With this option we just have to mention the keyword as “GROUP BY ALL” and that is it we do not have to worry about if we are keeping all column in the GROUP BY clause, or even if we add/forget anything this should take care of it. Below is how it can be used:</p><pre>--Below is how we can use the GROUP BY ALL clause.<br>SELECT <br>    O_ORDERSTATUS, <br>    O_ORDERPRIORITY, <br>    SUM(O_TOTALPRICE) <br>FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF10.ORDERS <br>GROUP BY ALL;<br><br><br>-- Some of the caveats like we cannot combine other columns alongwith GROUP BY ALL:<br>SELECT <br>    O_ORDERSTATUS, <br>    O_ORDERPRIORITY, <br>    SUM(O_TOTALPRICE) <br>FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF10.ORDERS <br>GROUP BY ALL, O_ORDERSTATUS;           -----&gt; THIS WOULD FAIL.</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/992/1*n8lut9VrHPzygyILd7bUtw.png" /><figcaption>GROUP BY ALL- Use cases.</figcaption></figure><h4>6. SELECT * “ILIKE”</h4><p>ILIKE with SELECT clause can become very handy when we want to SELECT only limited columns with some pattern from a table. Imagine that we are attempting to obtain every column whose name contains the word “order” and then fire query. The ILIKE keyword is useful in this situation.</p><p>While the percent sign (%) can be used to match any sequence of zero or more characters, the underscore (_) symbol can match any single character. The matching procedure for patterns is case-insensitive, as the ILIKE operator implies.</p><pre><br>--Below is how we just get the column names which has &quot;order&quot; key word in it.<br>SELECT * ILIKE &#39;%order%&#39; <br>FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF10.ORDERS <br>LIMIT 10;<br><br><br>--Another way to get the output with pattern matching.<br>SELECT * ILIKE &#39;%_r&#39; <br>FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF10.ORDERS <br>LIMIT 10;<br></pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/991/1*CyJfXtytc4mCxGuL9VuIQQ.png" /><figcaption>Some of the use cases of ILIKE</figcaption></figure><h4>7. IF EXISTS, IF NOT EXISTS</h4><p>For developers and database managers, adding or removing columns is a standard procedure. However, there are times when we should only add a column if it isn’t there already or remove a column if it is. Snowflake extends the “IF [NOT] EXISTS” clause to the column level for this reason.</p><p>Let us see them as an example:</p><p>Consider the below table as my BASE table:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xmFGW9qHoxbozl7BjMfr2w.png" /><figcaption>We would use this as a reference</figcaption></figure><pre>-- To add a new column only if it do not exist::<br>ALTER TABLE EMPLOYEE_TABLE ADD COLUMN IF NOT EXISTS DEPARTMENT_NAME VARCHAR;<br>  --&gt; As we saw the column &quot;department_name&quot; was not present and hence it would get added.<br><br>-- To drop an existing column only if it’s present::<br>ALTER TABLE EMPLOYEE_TABLE DROP COLUMN IF EXISTS LAST_NAME;<br>--&gt; As we saw the column &quot;LAST_NAME&quot; was present and hence it would get dropped.</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/996/1*87HfMtl_S40w8xhT1tHpHA.png" /><figcaption>Some of the use cases.</figcaption></figure><h4>8. ARRAY_SORT, ARRAY_MIN, ARRAY_MAX</h4><p>These functions are really essential when we want to process the array data in an ordered way. Also these functions have the potential to consider NULL as one of there comparator operators and accordingly gives the output. Some examples are given as below:</p><pre><br>--This query would give elements of an input array in a sorted manner.<br>SELECT ARRAY_SORT(ARRAY_GENERATE_RANGE(0,100,20), FALSE);<br>   o/p:: [80,60,40,20,0]<br><br>--This query would maximum value in this element even if we have NULLs in it.<br>SELECT ARRAY_MAX([20, 0, NULL, 10, NULL]);<br>    o/p:: 20<br><br>--This query would give minium value in this element even if we have NULLs in it.<br>SELECT ARRAY_MIN([20, 0, NULL, 10, NULL]);<br>    o/p:: 10</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/994/1*ILxSlQsscQiIEKURV9qjrQ.png" /><figcaption>Use cases</figcaption></figure><h4>9. BANKER’s ROUNDING</h4><p>Banker’s rounding or the option “<strong>rounding half to even</strong>” is something which is relatively new in Snowflake. Folks would now have the option to select between rounding half to even and this rounding mode with the addition of the new parameter. This is used in conjuction with Snowflake’s ROUND() function. This function allows to choose between 2 options i.e., “HALF_TO_EVEN” and “HALF_AWAY_FROM_ZERO”. Let us see some of the examples:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Wps11WXKRrnAWVhjHWVHPw.png" /><figcaption>Example-1</figcaption></figure><p>If we see above, then HALF_TO_EVEN gives us the value as “2” which is a nearest even number of 2.5</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9GlXRG-m8zGHJ18YJkUKiA.png" /><figcaption>Example-2</figcaption></figure><p>If we see above, then HALF_TO_EVEN gives us the value as “4” which is a nearest even number of 3.5</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/990/1*45pK0Ubg7nRqoq6KwpXz8w.png" /><figcaption>Use cases</figcaption></figure><h4>10. Keyword “TABLE” with new capability</h4><p>We know that “SELECT *” gives us the output from the table by giving us all the columns. Now we have an alternative as well i.e., just “<strong>TABLE</strong>”. Below is the example:</p><pre>SELECT * FROM EMPLOYEE_TABLE; --- This is the conventional way.<br><br>TABLE EMPLOYEE_TABLE; --- This is the new way. The o/p is still going to be the same.</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nf15igId--kkVqV6iFew-w.png" /><figcaption>The o/p</figcaption></figure><p>Let us now discuss some of the <strong><em>PRIVATE PREVIEW</em></strong> features on Snowflake . These mostly can be used so that it can be used as DataOps or DevOps use cases.</p><h4>“<strong>CREATE OR ALTER TABLE</strong>”:</h4><p>Now, this is feature i.e., CREATE OR ALTER TABLE where we keep on adding new columns is a private preview feature. This command is quite helpful when dealing with slowly changing schemas, particularly when there is ambiguity about a table’s current state. The benefit is that you don’t have to start from scratch every time you make a table or figure out exactly which ALTER has to be done.</p><pre>/*This is how the syntax is expected to work. <br>/*Please note it is a Private preview feature hence we need follow Snowflake to get more insights about it. */<br><br>CREATE OR ALTER TABLE EMPLOYEE_TABLE(<br>  emp_id NUMBER,<br>  emp_name VARCHAR,<br>  salary NUMBER<br>);</pre><h3>SUMMARY:</h3><p>I have shared some of the key features which are essentially new within Snowflake and also given some perspective on the potential use cases that these features solves. Please note that Snowflake is an well established and also an evolving platform for good. Hence actively following them and getting to know about all new features &amp; capabilities helps in solving varied use cases.</p><p><strong>Please keep reading my blogs it is only going to encourage me in posting more such content. You can find me on LinkedIn by clicking </strong><a href="https://www.linkedin.com/in/somen-swain-a03625177">here</a><strong> and on Medium </strong><a href="https://medium.com/@somen.swain"><strong>here</strong></a><strong>. Happy Learning :)</strong></p><p>Awarded consecutively as <strong><em>“Data Superhero by Snowflake for year 2024 &amp; 2023”. Links: </em></strong><a href="https://medium.com/snowflake/introducing-the-2024-snowflake-data-superheroes-1962bc20079f">here</a></p><p><em>Disclaimer: The views expressed here are mine alone and do not necessarily reflect the view of my current, former, or future employers.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9fa28c64384b" width="1" height="1" alt=""><hr><p><a href="https://medium.com/snowflake/sql-snowflake-new-capabilities-9fa28c64384b">SQL &amp; Snowflake- New capabilities</a> was originally published in <a href="https://medium.com/snowflake">Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, &amp; Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Writing Robust Procedures with Snowpark]]></title>
            <link>https://medium.com/snowflake/writing-robust-procedures-with-snowpark-07c12b7650f2?source=rss----34b6daafc07---4</link>
            <guid isPermaLink="false">https://medium.com/p/07c12b7650f2</guid>
            <category><![CDATA[snowpark]]></category>
            <category><![CDATA[user-defined-functions]]></category>
            <category><![CDATA[snowflake]]></category>
            <category><![CDATA[stored-procedure]]></category>
            <category><![CDATA[native-app-development]]></category>
            <dc:creator><![CDATA[Luis Villavicencio]]></dc:creator>
            <pubDate>Thu, 25 Apr 2024 16:58:00 GMT</pubDate>
            <atom:updated>2024-04-25T16:58:00.546Z</atom:updated>
            <content:encoded><![CDATA[<p>Stored procedures are vital components for logic encapsulation, serving as fundamental building blocks for any Snowflake project. Whether for performing database operations or supporting Snowflake native applications, stored procedures are crucial.</p><p>Crafting robust stored procedures is imperative for efficiency, ease of understanding, troubleshooting, modifications, and scalability. While this article provides some guidance and key considerations to elevate your development in Snowflake with a focus on stored procedures and Snowpark for Python, most of the points described here also apply to Snowflake UDFs or any other Snowpark initiative.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/895/1*IBWyKqu6pUat8xK6EWyWyQ.png" /></figure><h3>Exception handling</h3><p>Take control of the execution by implementing error handling.</p><p>It is recommended to use <em>Try-Except-Finally</em> blocks to catch specific Exceptions and clean up your process with a <em>Finally</em> Block when needed. This practice allows you to handle various errors and resolutions more efficiently instead of having only one generic <em>Except</em> block.</p><p>Snowpark provides various <a href="https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/exceptions">Exception classes</a> for tailored handling. Here is an example:</p><pre>#The &#39;try&#39; block runs a block of code<br>#The &#39;except&#39; block handles any error from the &#39;try&#39; block.<br>#The &#39;else&#39; block executes code when there is no error.<br>#The &#39;finally&#39; block executes code, regardless of the result of the previous blocks.<br><br>(...)<br>from snowflake.snowpark.exceptions import SnowparkSessionException,SnowparkSQLException, SnowparkClientException<br>(...)<br>    try:<br>        #&lt;do something&gt;<br>    except SnowparkSessionException as e: # exception handling example<br>        print(&#39;EXCEPTION | Error on session connection : {} &#39;.format(e.message))<br>    except SnowparkSQLException as e: # exception handling example<br>        print(&#39;EXCEPTION | Error on SQL : {} &#39;.format(e.message)) <br>        print(&#39;Query_id : {} &#39;.format(e.sfqid)) <br>    except SnowparkClientException as e: # exception handling example<br>        print(&#39;EXCEPTION | Something didnt work on snowpark: {} &#39;.format(e.message)) <br>    except Exception as e: #cathing all errors, it can be anything else<br>        output_message = &#39;EXCEPTION-PYTHON | Something didnt work on Stored Procedure:&#39;<br>    else :<br>        print(&#39;Nothing when wrong!&#39;) <br>    finally: <br>        #session.close() <br>        #pass<br>(...)</pre><p>Additional examples <a href="https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-example#handling-errors">here</a></p><h3>Logging</h3><p>Implement logging to capture relevant information for debugging and monitoring.</p><p>With logging we can record events that happen during the code execution, this a native Python example:</p><pre># importing module<br>import logging<br><br># Creating the logger object<br>logger = logging.getLogger(&quot;mylog&quot;)<br><br># Setting the threshold of logger to DEBUG<br>logger.setLevel(logging.DEBUG)<br><br># logs message at debug level<br>logger.debug(&quot;This is the first debug message&quot;)<br>logger.debug(&quot;This is the second debug message&quot;)</pre><p>The Snowflake <a href="https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-operations">Event tables</a> feature is designed to help you with this task by capturing the log messages and saving them in a table (event table), allowing you to query it for future needs (eg: troubleshooting, performance insights, etc).</p><p>It is important to be careful when writing logging messages to avoid revealing sensitive information. If you need to share an event with colleagues, make sure that you have set up <a href="https://docs.snowflake.com/en/user-guide/security-access-control-overview">Role-Based Access Control (RBAC)</a> properly. To protect personal information, <a href="https://docs.snowflake.com/en/sql-reference/sql/create-view">Create a view</a> of the event table and <a href="https://docs.snowflake.com/en/sql-reference/sql/create-masking-policy">apply masking policies</a> to it. This will ensure that personal identifiable information (PII) is either masked or removed.</p><p>This is how it will look like within a store procedure:</p><pre>CREATE OR REPLACE PROCEDURE do_something()<br>RETURNS VARCHAR<br>LANGUAGE PYTHON<br>PACKAGES=(&#39;snowflake-snowpark-python&#39;)<br>RUNTIME_VERSION=3.8<br>HANDLER=&#39;run&#39;<br>AS $$<br>import logging<br><br>logger = logging.getLogger(&quot;mylog&quot;)<br>logger.info(&quot;Logging from Python&quot;)<br><br>def run(session):<br>  logger.info(&quot;function start&quot;)<br>  try:<br>    #&lt;do_something&gt;<br>    x = my_private_data<br>  except Exception:<br>    logger.error(&quot;Logging an error from Python&quot;)<br>    logger.error(&quot;my_private_data is : &quot; + my_private_data) # THIS IS A VERY BAD EXAMPLE<br>    return &quot;ERROR&quot;<br>  return &quot;SUCCESS&quot;<br><br>$$;</pre><p>You will find the logs message in the event table , this table has a <a href="https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-columns">predefined set of columns</a>. Please refer to <a href="https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-setting-up">this</a> documentation for more details about the configuration and examples.</p><h3>Tracing</h3><p>While with Logging, you can manually capture point-in-time messages, with tracing, you can automatically capture the flow of code execution (span).</p><p>Event tracing can be valuable and contribute to the maintainability and resilience of production Stored Procedures, but extensive use of tracing might introduce overhead and produce a performance degradation.</p><p>You can find various examples for tracing <a href="https://docs.snowflake.com/en/developer-guide/logging-tracing/tracing-python">here</a> by using <a href="https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-operations">Event tables</a>.</p><h3>Parameterization</h3><p>Ensure Flexibility by designing procedures that can easily adapt to new parameters with minimal disruption.</p><p>A good option is to leverage Snowflake’s<a href="https://docs.snowflake.com/en/user-guide/semistructured-intro#storing-semi-structured-data"> semi-structured</a> data types and<a href="https://docs.snowflake.com/en/sql-reference/functions-semistructured"> Semi-structured and Structured Data Functions</a>.</p><p>As an example, the following stored procedure receives the input parameters in JSON format (Snowflake Variant data type) while parameter order is not enforced:</p><pre>CREATE OR REPLACE PROCEDURE MY_DB.MY_SCHEMA.MY_STORED_PROCEDURE(input_configuration_options variant)<br>RETURNS STRING<br>LANGUAGE PYTHON<br>RUNTIME_VERSION = &#39;3.8&#39;<br>PACKAGES = (&#39;snowflake-snowpark-python&#39;)<br>HANDLER = &#39;run&#39;<br>EXECUTE AS CALLER <br>AS<br>$$<br><br>def run(session, input_configuration_options):<br>       -- You can access the input variables as follows :<br>       return input_configuration_options[&#39;MY_INPUT1&#39;] <br>       --Or also : input_configuration_options.get(&#39;MY_INPUT1&#39;)<br>       (...)<br>$$;</pre><p>One of many ways to call the previous procedure is :</p><pre>CALL MY_DB.MY_SCHEMA.MY_STORED_PROCEDURE(PARSE_JSON(<br>  $$<br>    {<br>                &#39;MY_INPUT1&#39;     : &quot;STRING_VALUE1&quot;,<br>                &#39;MY_INPUT2&#39;     : &quot;STRING_VALUE1&quot; ,<br>                &#39;MY_INPUT3&#39;     : True<br>    }<br>  $$<br>));</pre><h3>Input Validation</h3><p>Validate input parameters against expected criteria to reduce the risk of errors or unexpected results.</p><p>To ensure consistency between different languages, review the Snowflake<a href="https://docs.snowflake.com/en/developer-guide/udf-stored-procedure-data-type-mapping#sql-python-data-type-mappings"> data type mapping</a>. The following example demonstrates how to validate data types and mandatory parameters.</p><pre>CREATE OR REPLACE PROCEDURE  MY_DB.MY_SCHEMA.MY_STORED_PROCEDURE(input_configuration_options variant)<br>RETURNS STRING<br>LANGUAGE PYTHON<br>RUNTIME_VERSION = &#39;3.8&#39;<br>PACKAGES = (&#39;snowflake-snowpark-python&#39;)<br>HANDLER = &#39;run&#39;<br>EXECUTE AS CALLER <br>AS<br>$$<br><br>def run(session, input_configuration_options):<br><br>    expected_configuration_options = {<br>        &#39;MY_INPUT1&#39;     : {&quot;data_type&quot; : str  ,  &quot;is_mandatory&quot; : True  },<br>        &#39;MY_INPUT2&#39;     : {&quot;data_type&quot; : str  ,  &quot;is_mandatory&quot; : True  }, <br>        &#39;MY_INPUT3&#39;     : {&quot;data_type&quot; : bool , &quot;is_mandatory&quot;  : False, &quot;default&quot; : False }<br>    }<br><br>    -- Parameter validation<br>    -- NOTE :  Parameters are case-sensitive in this example<br>    for option, option_info in expected_configuration_options.items():<br>        if option in input_configuration_options.keys():<br>        -- Checking data type<br>            if  not isinstance(input_configuration_options.get(option),option_info[&#39;data_type&#39;]):<br>                output_message = &quot;Error: Wrong data type. {} shoulb be {}&quot;.format(option,option_info[&#39;data_type&#39;])<br>                return output_message   <br>        -- Checking mandatory parameters<br>        elif option_info[&#39;is_mandatory&#39;]:<br>            output_message = &quot;Error: Madatory input parameter {} missing&quot;.format(option)<br>            return output_message<br>        -- Set default value, not mandatory and not in input <br>        else :<br>            input_configuration_options[option] = option_info[&#39;default&#39;]<br><br>    (...)<br><br>$$;</pre><h3>Documentation</h3><p>In addition to the comments you can add to the code, supplement it with<a href="https://docs.snowflake.com/en/sql-reference/sql/comment"> object comments</a>, and provide a high-level description of the stored procedure.</p><p>In the context of Stored procedures, Snowflake provides<a href="https://docs.snowflake.com/en/sql-reference/info-schema/procedures"> PROCEDURES View</a>, among many Others. Reading the <em>comment</em> field (From PROCEDURES View) can be particularly helpful for anyone who may be reviewing or governing company procedures.</p><h3>Testing</h3><p>Include comprehensive testing. Refer to <a href="https://docs.pytest.org/en/7.4.x/">PyTest documentation </a>for writing tests. Some useful documentation and examples <a href="https://docs.snowflake.com/en/developer-guide/snowpark/python/testing-python-snowpark">here</a>.</p><p>In addition, consider performing performance and concurrence testing.</p><h3>Performance and optimization</h3><p>This is a very wide topic. Choosing the most appropriate implementation approach can be challenging. On the one hand, factors such as time and space complexity are part of every programming language. On the other hand, we have<a href="https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/index"> Snowpark methods</a> along with the rest of Snowflake features that have to be used attentively. Here is some advice to help with this task:</p><ul><li>Verify the<a href="https://docs.snowflake.com/en/user-guide/ui-query-profile"> query profile</a> of the queries generated from your stored procedure.</li><li>Avoid redundant or expensive computations in your algorithm (e.g., Row by row processing loop, exploding joins, Unnecessary and complex Regex, etc).</li><li>Implement lazy evaluation. Push down processing to Snowflake Virtual Warehouse and Delay the use of Snowpark Dataframe actions that cause data to be computed and materialized on the client side until needed. (eg: Collect(), to_pandas())</li><li>Consider a batch processing approach.</li></ul><p>Some extra applicable documentation:</p><ul><li><a href="https://docs.snowflake.com/en/guides-overview-performance">Optimizing performance in Snowflake | Snowflake Documentation</a></li><li><a href="https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-batch">Vectorized Python UDFs | Snowflake Documentation</a></li><li><a href="https://quickstarts.snowflake.com/guide/snowpark_python_top_three_tips_for_optimal_performance">Snowpark Python: Top Three Tips for Optimal Performance</a></li></ul><h3>Concurrency</h3><p>If applicable, implement mechanisms to handle scenarios where multiple instances of the same procedure run simultaneously.</p><p>Concurrent Stored Procedures can lead to unwanted references to objects or artifacts created by the other concurrent Execution, producing unexpected results, like data duplication, object or content removal or alteration, etc.</p><h3>Summary</h3><p>This small guide covers various aspects that can contribute to the efficiency, effectiveness, and maintainability of Snowflake stored procedures or UDFs. While some points can be easily implemented and standardized (e.g., Documentation, input validation), others require more attention and expertise (e.g., Performance optimization, testing).</p><p>Consider including the following checklist in your development process:</p><p>[ ] Exception handling : Snowpark <a href="https://docs.snowflake.com/ko/developer-guide/snowpark/reference/python/latest/exceptions">Exceptions</a> classes<br>[ ] Logging : Snowflake <a href="https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-operations">Event tables</a><br>[ ] Tracing : Snowflake <a href="https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-operations">Event tables</a><br>[ ] Parameterization : Snowflake <a href="https://docs.snowflake.com/en/sql-reference-data-types">Data types</a> and <a href="https://docs.snowflake.com/en/sql-reference/functions-semistructured">Semi-structured and Structured Data Functions</a><br>[ ] Input Validation : Snowflake <a href="https://docs.snowflake.com/en/sql-reference-data-types">Data types</a> and Data type <a href="https://docs.snowflake.com/en/developer-guide/udf-stored-procedure-data-type-mapping#sql-python-data-type-mappings">mapping</a><br>[ ] Documentation : Snowflake Object <a href="https://docs.snowflake.com/en/sql-reference/sql/comment">Comment</a> and <a href="https://docs.snowflake.com/en/sql-reference/info-schema/procedures">Procedures View</a><br>[ ] Testing : <a href="https://docs.snowflake.com/en/developer-guide/snowpark/python/testing-python-snowpark">Tests for Snowpark Python </a>and any other python library like <a href="https://docs.snowflake.com/en/developer-guide/snowpark/python/testing-python-snowpark#integration-tests-for-stored-procedures">PyTest</a><br>[ ] Performance and optimization : Snowflake <a href="https://docs.snowflake.com/en/user-guide/ui-query-profile">Query Profile</a>. Snowpark <a href="https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/api/snowflake.snowpark.DataFrame">Lazy Dataframes</a> support.<br>[ ] Concurrency : Snowpark <a href="https://docs.snowflake.com/ko/developer-guide/snowpark/reference/python/latest/exceptions">Exceptions</a> classes and native python control flows.</p><p>Additionally, you can use the Snowflake <a href="https://docs.snowflake.com/en/guides-overview-alerts">Alerts and Notification</a> feature to send emails or trigger automatic incidents (e.g., Trigger alerts based on event tables mentioned above, Based on Procedures View, or any other S<a href="https://docs.snowflake.com/en/sql-reference/account-usage">nowflake metadata</a>, data quality checks, etc.).</p><p>Finally, in the era of AI, keep an eye open for the latest Snowflake innovations that can help you in your day-to-day (They are in preview at the moment of writing this article):<a href="https://docs.snowflake.com/en/user-guide/snowflake-copilot"> Copilot</a>,<a href="https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/"> Arctic</a> (use<a href="https://arctic.streamlit.app/"> this</a> app!)</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=07c12b7650f2" width="1" height="1" alt=""><hr><p><a href="https://medium.com/snowflake/writing-robust-procedures-with-snowpark-07c12b7650f2">Writing Robust Procedures with Snowpark</a> was originally published in <a href="https://medium.com/snowflake">Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, &amp; Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>